{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a_8qcu1EWHr"
      },
      "outputs": [],
      "source": [
        "pip install -U huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = 'hf_'"
      ],
      "metadata": {
        "id": "XAr4QjM2EXUn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from os.path import basename\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "GITHUB_URL = \"https://github.com/GwenTsang/concours-de-philosophie/blob/main/copies_json_csv/L%20idee%20de%20perfection.json\"\n",
        "\n",
        "def download(u):\n",
        "    u = u.replace(\"github.com/\", \"raw.githubusercontent.com/\").replace(\"/blob/\", \"/\")\n",
        "    r = requests.get(u, headers={\"User-Agent\": \"curl/8\"}, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    filename = basename(urlparse(u).path)\n",
        "    open(filename, \"wb\").write(r.content)\n",
        "    return filename\n",
        "\n",
        "download(GITHUB_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QYvWjS8GJxdz",
        "outputId": "c1fbe38c-2d48-41d4-f1bd-39f1012e2853"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'L%20idee%20de%20perfection.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "def extract_partie_1(data: dict) -> str:\n",
        "    \"\"\"Extrait le champ 'partie_1' du JSON.\"\"\"\n",
        "    if \"partie_1\" not in data:\n",
        "        raise KeyError(\"Le champ 'partie_1' est manquant dans le JSON.\")\n",
        "    value = data[\"partie_1\"]\n",
        "    # Si jamais la partie est stockée sous forme de liste/parags, on la join proprement\n",
        "    if isinstance(value, list):\n",
        "        value = \"\\n\\n\".join(str(x) for x in value)\n",
        "    else:\n",
        "        value = str(value)\n",
        "    return value.strip()\n",
        "\n",
        "def main():\n",
        "    # 1) Charger le fichier JSON\n",
        "    data_path = \"/content/L%20idee%20de%20perfection.json\"\n",
        "\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # 2) Extraire \"partie_1\"\n",
        "    texte = extract_partie_1(data)\n",
        "\n",
        "    # 3) Envoyer à un LLM via Hugging Face InferenceClient\n",
        "    client = InferenceClient(\n",
        "        provider=\"publicai\",\n",
        "        api_key=os.environ[\"HF_TOKEN\"]\n",
        "    )\n",
        "\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Fais la liste des œuvres mobilisées dans le texte qui suit. Répond uniquement avec les titres des œuvres écrites en italique en markdown comme dans le texte. N'ajoute aucune remarque, rien. Texte à analyser : {texte}\",\n",
        "    }]\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"swiss-ai/Apertus-70B-Instruct-2509\",\n",
        "        messages=messages,\n",
        "    )\n",
        "\n",
        "    # Affichage de la réponse du modèle\n",
        "    # Selon la version du client, l'attribut peut être resp.choices[0].message ou .message.content\n",
        "    # On gère les deux pour plus de robustesse.\n",
        "    choice = resp.choices[0]\n",
        "    output = getattr(choice, \"message\", None)\n",
        "    if isinstance(output, dict) and \"content\" in output:\n",
        "        print(output[\"content\"])\n",
        "    else:\n",
        "        print(output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr7Z6kymINeI",
        "outputId": "5f7d4f98-9416-46b7-eb41-116fdd240a2d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- *Le Sophiste* - *Ménon* - *Confessions* - *Essais de Théodicée*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from os.path import basename\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "GITHUB_URL = \"https://github.com/GwenTsang/concours-de-philosophie/blob/main/copies_json_csv/la_servitude_volontaire.json\"\n",
        "\n",
        "def download(u):\n",
        "    u = u.replace(\"github.com/\", \"raw.githubusercontent.com/\").replace(\"/blob/\", \"/\")\n",
        "    r = requests.get(u, headers={\"User-Agent\": \"curl/8\"}, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    filename = basename(urlparse(u).path)\n",
        "    open(filename, \"wb\").write(r.content)\n",
        "    return filename\n",
        "\n",
        "download(GITHUB_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GpzVYSd2Kszs",
        "outputId": "8213361f-8e3c-40af-ab91-518f643addc2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'la_servitude_volontaire.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "def extract_partie_1(data: dict) -> str:\n",
        "    \"\"\"Extrait le champ 'partie_1' du JSON.\"\"\"\n",
        "    if \"partie_1\" not in data:\n",
        "        raise KeyError(\"Le champ 'partie_1' est manquant dans le JSON.\")\n",
        "    value = data[\"partie_1\"]\n",
        "    # Si jamais la partie est stockée sous forme de liste/parags, on la join proprement\n",
        "    if isinstance(value, list):\n",
        "        value = \"\\n\\n\".join(str(x) for x in value)\n",
        "    else:\n",
        "        value = str(value)\n",
        "    return value.strip()\n",
        "\n",
        "def main():\n",
        "    # 1) Charger le fichier JSON\n",
        "    data_path = \"/content/la_servitude_volontaire.json\"\n",
        "\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # 2) Extraire \"partie_1\"\n",
        "    texte = extract_partie_1(data)\n",
        "\n",
        "    # 3) Envoyer à un LLM via Hugging Face InferenceClient\n",
        "    client = InferenceClient(\n",
        "        provider=\"publicai\",\n",
        "        api_key=os.environ[\"HF_TOKEN\"]\n",
        "    )\n",
        "\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Fais la liste des œuvres mobilisées dans le texte qui suit. Répond uniquement avec les titres des œuvres écrites en italique en markdown comme dans le texte. N'ajoute aucune remarque, rien. Texte à analyser : {texte}\",\n",
        "    }]\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"swiss-ai/Apertus-70B-Instruct-2509\",\n",
        "        messages=messages,\n",
        "    )\n",
        "\n",
        "    # Affichage de la réponse du modèle\n",
        "    # Selon la version du client, l'attribut peut être resp.choices[0].message ou .message.content\n",
        "    # On gère les deux pour plus de robustesse.\n",
        "    choice = resp.choices[0]\n",
        "    output = getattr(choice, \"message\", None)\n",
        "    if isinstance(output, dict) and \"content\" in output:\n",
        "        print(output[\"content\"])\n",
        "    else:\n",
        "        print(output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKDIOKWlKyhU",
        "outputId": "3adcdbf3-dd63-4d36-e978-b59dc9b58a7f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*Discours de la servitude volontaire*  \n",
            "*Le deuxième sexe*\n"
          ]
        }
      ]
    }
  ]
}