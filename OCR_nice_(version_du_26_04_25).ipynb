{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86680641ed9b4b4884e323847b9598df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0b9f09ead864643b4a577e04038e935",
              "IPY_MODEL_5a1f269a69174a1787ce6c2c7f128c8d",
              "IPY_MODEL_086e6673b4524ce49fcda6175986536f"
            ],
            "layout": "IPY_MODEL_f705514cde3e485186a48cb54714619b"
          }
        },
        "c0b9f09ead864643b4a577e04038e935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7def905f5b75438f9429ad2b3bbcc043",
            "placeholder": "​",
            "style": "IPY_MODEL_f0de132605024195bcb07a546f4e607f",
            "value": "Processing Chunks: 100%"
          }
        },
        "5a1f269a69174a1787ce6c2c7f128c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d895ce991224244badbab1b7316bb56",
            "max": 45,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42311551eda74508a56ea9e329526c90",
            "value": 45
          }
        },
        "086e6673b4524ce49fcda6175986536f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e4d61b22c5406c95adc21248e3eeeb",
            "placeholder": "​",
            "style": "IPY_MODEL_ce9e8d572423473a90cd60cbf201b36f",
            "value": " 45/45 [10:25&lt;00:00, 14.24s/it]"
          }
        },
        "f705514cde3e485186a48cb54714619b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7def905f5b75438f9429ad2b3bbcc043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0de132605024195bcb07a546f4e607f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d895ce991224244badbab1b7316bb56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42311551eda74508a56ea9e329526c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0e4d61b22c5406c95adc21248e3eeeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce9e8d572423473a90cd60cbf201b36f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcb0257035954fd4802695e50bc1089d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1846e71737e54dcfb06cda1e58684d0d",
              "IPY_MODEL_7912c9de38aa44f4ad49d8c766558955",
              "IPY_MODEL_fdc466b072d44e858c1cf1ab232e6a3b"
            ],
            "layout": "IPY_MODEL_8c2919ec86f34d67b08990ad3a9757cc"
          }
        },
        "1846e71737e54dcfb06cda1e58684d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f6410cd968d405d9aae8f3fa1ce7d6b",
            "placeholder": "​",
            "style": "IPY_MODEL_7d5988e0cb714526aa4e185ce932bb8c",
            "value": "Processing Chunks: 100%"
          }
        },
        "7912c9de38aa44f4ad49d8c766558955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91935e78dbba43b0987af5f9b868fb1c",
            "max": 56,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab8b92233b094267b1824c52c98ab75a",
            "value": 56
          }
        },
        "fdc466b072d44e858c1cf1ab232e6a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_643a1c6961014f12b12b9c4aae310112",
            "placeholder": "​",
            "style": "IPY_MODEL_c74595b9b95a49b3853bb7a9702be7c7",
            "value": " 56/56 [11:34&lt;00:00, 11.38s/it]"
          }
        },
        "8c2919ec86f34d67b08990ad3a9757cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f6410cd968d405d9aae8f3fa1ce7d6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d5988e0cb714526aa4e185ce932bb8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91935e78dbba43b0987af5f9b868fb1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab8b92233b094267b1824c52c98ab75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "643a1c6961014f12b12b9c4aae310112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c74595b9b95a49b3853bb7a9702be7c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "093b38b98c3d4473be2021c7f6ee648b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1288ee98ac4242dbb5c011bf3d1f5851",
              "IPY_MODEL_87367c60d82c4b34b576a2453dcdbef3",
              "IPY_MODEL_89441e59841847f1aa1df6b22e640468"
            ],
            "layout": "IPY_MODEL_5249e308bda847888542129f63d601dd"
          }
        },
        "1288ee98ac4242dbb5c011bf3d1f5851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_885087257319416c8ce7e0338a0d9b95",
            "placeholder": "​",
            "style": "IPY_MODEL_491137e96d904aec8bd4de162aa6cb0a",
            "value": "Processing Chunks: 100%"
          }
        },
        "87367c60d82c4b34b576a2453dcdbef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9154ee28657c4fb286ef4b026c5c6867",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79051d52de5e4e92bbbb01df89ec731c",
            "value": 43
          }
        },
        "89441e59841847f1aa1df6b22e640468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d77957e1019f4d3484374282d7e26f8d",
            "placeholder": "​",
            "style": "IPY_MODEL_0eacf43db0e242478ce4df17cecd36e1",
            "value": " 43/43 [11:38&lt;00:00, 15.15s/it]"
          }
        },
        "5249e308bda847888542129f63d601dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "885087257319416c8ce7e0338a0d9b95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "491137e96d904aec8bd4de162aa6cb0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9154ee28657c4fb286ef4b026c5c6867": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79051d52de5e4e92bbbb01df89ec731c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d77957e1019f4d3484374282d7e26f8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eacf43db0e242478ce4df17cecd36e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8707f2c37cb44e3d9cd1c19d2d826b57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0a100db45054d6a832e159cd2b1da79",
              "IPY_MODEL_be9361b072d240b5bac76580857dba76",
              "IPY_MODEL_4c5e5642f1ac4fb0ac5371a623ab2612"
            ],
            "layout": "IPY_MODEL_886894204e2843c4b41350ec06bcc72b"
          }
        },
        "e0a100db45054d6a832e159cd2b1da79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f304d4444eb4f0c923e25a07873da68",
            "placeholder": "​",
            "style": "IPY_MODEL_5014923c3eb94630939c1a9db0e76893",
            "value": "Processing Chunks:  79%"
          }
        },
        "be9361b072d240b5bac76580857dba76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa6bf4c563644c0bd1ab8ce82dbe3fa",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8143492a567a49169a989f9039319369",
            "value": 41
          }
        },
        "4c5e5642f1ac4fb0ac5371a623ab2612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa22cc218a8445f7a23e280d8d971eea",
            "placeholder": "​",
            "style": "IPY_MODEL_c0c12db4fa13407b93de9b41e6d64aeb",
            "value": " 41/52 [08:02&lt;00:56,  5.18s/it]"
          }
        },
        "886894204e2843c4b41350ec06bcc72b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f304d4444eb4f0c923e25a07873da68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5014923c3eb94630939c1a9db0e76893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8aa6bf4c563644c0bd1ab8ce82dbe3fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8143492a567a49169a989f9039319369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa22cc218a8445f7a23e280d8d971eea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c12db4fa13407b93de9b41e6d64aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73c4b6cfcda2494eb21ce068305b1280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6abb5c3be45c4d6c949c84a71c5abdc1",
              "IPY_MODEL_78db113cb3a44bf4a698ff8b10e599f6",
              "IPY_MODEL_07754be44120482fb339b951249e6aa9"
            ],
            "layout": "IPY_MODEL_d86c690556704626ad52ad52b883e843"
          }
        },
        "6abb5c3be45c4d6c949c84a71c5abdc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8ee4fcfb46544eca39974b49e1d05ad",
            "placeholder": "​",
            "style": "IPY_MODEL_70d5da0408e84e5e8f2a128d29576ea5",
            "value": "Processing Chunks: 100%"
          }
        },
        "78db113cb3a44bf4a698ff8b10e599f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b59fc2c3a4ce4f9cb7bea65354c010aa",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba66cbc728c94a22b5dd36b29a442dbf",
            "value": 52
          }
        },
        "07754be44120482fb339b951249e6aa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a432ccc1e14a0691446c652e657fed",
            "placeholder": "​",
            "style": "IPY_MODEL_558ad718b9894647b578fa3951f8395f",
            "value": " 52/52 [11:09&lt;00:00, 12.20s/it]"
          }
        },
        "d86c690556704626ad52ad52b883e843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8ee4fcfb46544eca39974b49e1d05ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70d5da0408e84e5e8f2a128d29576ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b59fc2c3a4ce4f9cb7bea65354c010aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba66cbc728c94a22b5dd36b29a442dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71a432ccc1e14a0691446c652e657fed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "558ad718b9894647b578fa3951f8395f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fa8daadea2b47c5bcdfb442cfc2e7a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ab72cc74d7e4676920c8d5962c0e6db",
              "IPY_MODEL_f0705ec542e649bfa26bb4b007105a3d",
              "IPY_MODEL_a1a58928ba7c4a1fbdbc7797b1ce998e"
            ],
            "layout": "IPY_MODEL_11f6a0bf1b004efcb68ac9c4302a9ba2"
          }
        },
        "3ab72cc74d7e4676920c8d5962c0e6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76b996a4c076474e89fcb949113fb8ad",
            "placeholder": "​",
            "style": "IPY_MODEL_fe4b1bbb0e854dfabdf13f3e643e6388",
            "value": "Processing Chunks: 100%"
          }
        },
        "f0705ec542e649bfa26bb4b007105a3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_338f2eea4c8e4a5fb07452ef340047cb",
            "max": 51,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_819c99a985974144b5eb5a94cfa36471",
            "value": 51
          }
        },
        "a1a58928ba7c4a1fbdbc7797b1ce998e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4f5ba1d21a34604ac502272466e2026",
            "placeholder": "​",
            "style": "IPY_MODEL_4c3d4dd166754e2e8fa292aa195b192c",
            "value": " 51/51 [11:22&lt;00:00, 13.22s/it]"
          }
        },
        "11f6a0bf1b004efcb68ac9c4302a9ba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76b996a4c076474e89fcb949113fb8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe4b1bbb0e854dfabdf13f3e643e6388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "338f2eea4c8e4a5fb07452ef340047cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "819c99a985974144b5eb5a94cfa36471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4f5ba1d21a34604ac502272466e2026": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c3d4dd166754e2e8fa292aa195b192c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80059af4ffca4ebaaad819abff66daa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b52a62464491411088075b1ed54de551",
              "IPY_MODEL_b2b76ed0dd254f69b298001dc4decbca",
              "IPY_MODEL_bbc5e49b3114456e92c1dbabb9db0bac"
            ],
            "layout": "IPY_MODEL_3fbf65add74a41019f8c1ee0fa0f8960"
          }
        },
        "b52a62464491411088075b1ed54de551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b47d45a2fc62403d88b3c3fb45ff25d1",
            "placeholder": "​",
            "style": "IPY_MODEL_8746f79639ce4915afcfdb57f7f41896",
            "value": "Processing Chunks: 100%"
          }
        },
        "b2b76ed0dd254f69b298001dc4decbca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_906b0651be4549a5830cfbcd932fa96c",
            "max": 42,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b20e3ce6c0e44f16a28039fa03ecd0bf",
            "value": 42
          }
        },
        "bbc5e49b3114456e92c1dbabb9db0bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95ae33f666b74a37bff42db73e43787b",
            "placeholder": "​",
            "style": "IPY_MODEL_5873a05d94ce47589b19de739a9b44cf",
            "value": " 42/42 [09:42&lt;00:00, 11.29s/it]"
          }
        },
        "3fbf65add74a41019f8c1ee0fa0f8960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b47d45a2fc62403d88b3c3fb45ff25d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8746f79639ce4915afcfdb57f7f41896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "906b0651be4549a5830cfbcd932fa96c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b20e3ce6c0e44f16a28039fa03ecd0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95ae33f666b74a37bff42db73e43787b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5873a05d94ce47589b19de739a9b44cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb80e83ef7e648c6b61eb499d904c98b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7d2c6ddd6464b91a083af755ddc36f1",
              "IPY_MODEL_8ba1ab979aa74dfa8e4afc02a8571c32",
              "IPY_MODEL_3421eb6875d94aad83a970632501e01d"
            ],
            "layout": "IPY_MODEL_56a97efb0a424de8af40320125d99275"
          }
        },
        "d7d2c6ddd6464b91a083af755ddc36f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7858b0458a8b4f0fbdc1a0e8cd61264a",
            "placeholder": "​",
            "style": "IPY_MODEL_6b800820ca5f418c99ea3ac9656e1bdb",
            "value": "Processing Chunks: 100%"
          }
        },
        "8ba1ab979aa74dfa8e4afc02a8571c32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba1c0a00440d40e7b55c12c9b6abeb97",
            "max": 34,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57b9a98902424e3e9142dfc2000bd7ec",
            "value": 34
          }
        },
        "3421eb6875d94aad83a970632501e01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1be6f25bf4204073bef3bce784f3be86",
            "placeholder": "​",
            "style": "IPY_MODEL_a65b951301ad47ca9108644c28c55fe2",
            "value": " 34/34 [06:19&lt;00:00, 10.93s/it]"
          }
        },
        "56a97efb0a424de8af40320125d99275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7858b0458a8b4f0fbdc1a0e8cd61264a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b800820ca5f418c99ea3ac9656e1bdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba1c0a00440d40e7b55c12c9b6abeb97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57b9a98902424e3e9142dfc2000bd7ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1be6f25bf4204073bef3bce784f3be86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a65b951301ad47ca9108644c28c55fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4d5ec62806b408e9455dc29437e6d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d27944631693451cb082ef01749e3c60",
              "IPY_MODEL_7761f1906d9b4e0dbc186f34d4f45d0a",
              "IPY_MODEL_6fc1031ecb1c4235a6577b5b39341d5d"
            ],
            "layout": "IPY_MODEL_342b8b884aff42738dd2be52a4d0b40b"
          }
        },
        "d27944631693451cb082ef01749e3c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6584c81f7484b859e116790252a0114",
            "placeholder": "​",
            "style": "IPY_MODEL_b4933d16168846e88cc8ccbfd7b929f0",
            "value": "Processing Chunks:  74%"
          }
        },
        "7761f1906d9b4e0dbc186f34d4f45d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f87a4847bb3445db0c51d3df64e20b2",
            "max": 39,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97200a866ebb4ed2ada45e9fd25c0c9e",
            "value": 29
          }
        },
        "6fc1031ecb1c4235a6577b5b39341d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14fb29bb808f42d08ef3878de5063bfd",
            "placeholder": "​",
            "style": "IPY_MODEL_c93332724b8f41bc9b46e578e3d829dd",
            "value": " 29/39 [05:11&lt;01:50, 11.00s/it]"
          }
        },
        "342b8b884aff42738dd2be52a4d0b40b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6584c81f7484b859e116790252a0114": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4933d16168846e88cc8ccbfd7b929f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f87a4847bb3445db0c51d3df64e20b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97200a866ebb4ed2ada45e9fd25c0c9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14fb29bb808f42d08ef3878de5063bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c93332724b8f41bc9b46e578e3d829dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlNSqjdNLJCs",
        "outputId": "30351be3-d897-447a-bb04-2ed6c2b893e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title Installations préalables\n",
        "\n",
        "!pip install -U -q \"google\"\n",
        "!pip install -U -q \"google.genai\"\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src = '/content/drive/MyDrive/folder_partitioned_merged/Le bonheur est-il affaire de hasard et de nécessité ? (AGREG interne -  note : 15,5)'\n",
        "dst = '/content/Le bonheur est-il affaire de hasard et de nécessité 15.5 ? (AGREG interne -  note : 15,5)'\n",
        "\n",
        "if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "shutil.copytree(src, dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "D5MwVAcYLx-U",
        "outputId": "97ed9318-32f8-4f9a-ae7a-407244d51af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Le bonheur est-il affaire de hasard et de nécessité 15.5 ? (AGREG interne -  note : 15,5)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src = '/content/drive/MyDrive/folder_partitioned_merged_clean'\n",
        "dst = '/content/main_folder'\n",
        "\n",
        "if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "shutil.copytree(src, dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g_GA2aODo6Dz",
        "outputId": "49724570-16eb-4bd1-cea1-1a78e20640f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/main_folder'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OCR SERIAL LOOP (26/04/25)\n",
        "\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import csv\n",
        "import time\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "import traceback\n",
        "\n",
        "# --- Configuration ---\n",
        "MAIN_FOLDER_PATH = \"/content/main_folder\"  # Main folder containing subfolders\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GEMINI_API_KEY_3\"])\n",
        "    print(\"Gemini API Key configured.\")\n",
        "except KeyError:\n",
        "    print(\"🛑 Error: GEMINI_API_KEY environment variable not set.\")\n",
        "except Exception as e:\n",
        "    print(f\"🛑 Error configuring Gemini API: {e}\")\n",
        "\n",
        "# Gemini Model Configuration\n",
        "GENERATION_CONFIG = {\n",
        "  \"temperature\": 0.2,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "MODEL_NAME = \"gemini-2.5-pro-preview-03-25\"\n",
        "\n",
        "# System Prompts (remain the same)\n",
        "SYSTEM_INSTRUCTION_WITH_CONTEXT = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\".\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\n",
        "Aide Contextuelle (Texte Précédent) : Pour t'aider à déchiffrer l'écriture, le texte qui précède *immédiatement* celui de l'image actuelle est fourni ci-dessous (séparé par '--- Texte Précédent ---'). Ce texte a été obtenu par OCR des sections précédentes de la dissertation. Le texte de l'image actuelle commence là où le texte précédent s'arrête (il peut y avoir un léger chevauchement de lignes dû au découpage des images).\n",
        "\n",
        "**Priorité :** Ta tâche principale reste de transcrire *exactement* ce que tu vois sur l'image actuelle. Utilise le texte précédent **uniquement** comme une aide *secondaire* pour résoudre des ambiguïtés ou déchiffrer des mots très difficiles. **Ne laisse PAS le texte précédent remplacer ou modifier ce qui est clairement visible sur l'image actuelle.** En cas de conflit entre le texte précédent et l'image, **la transcription fidèle de l'image PRÉVAUT.**\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_INSTRUCTION_FIRST_CHUNK = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\". Ceci est le tout début de la dissertation.\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\"\"\"\n",
        "\n",
        "# --- Functions ---\n",
        "\n",
        "def extract_folder_info(folder_path):\n",
        "    \"\"\"Extracts the full folder name while preserving all characters.\"\"\"\n",
        "    if not os.path.isdir(folder_path):\n",
        "        return None, None\n",
        "    full_name = os.path.basename(folder_path)\n",
        "    return full_name, full_name  # Return full name for both values\n",
        "\n",
        "# --- Function to sanitize filename ---\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Removes or replaces characters invalid for filenames.\"\"\"\n",
        "    # Remove leading/trailing whitespace\n",
        "    sanitized = filename.strip()\n",
        "    # Replace common invalid characters with underscore (adjust as needed)\n",
        "    sanitized = re.sub(r'[\\\\/*?:\"<>|]', '_', sanitized)\n",
        "    # Replace forward slash specifically (common in paths)\n",
        "    sanitized = sanitized.replace('/', '_')\n",
        "    # Optional: Limit length if necessary (e.g., 200 chars)\n",
        "    # max_len = 200\n",
        "    # if len(sanitized) > max_len:\n",
        "    #     name, ext = os.path.splitext(sanitized)\n",
        "    #     sanitized = name[:max_len - len(ext) - 1] + '~' + ext\n",
        "    return sanitized\n",
        "\n",
        "\n",
        "# ... (upload_to_gemini, delete_gemini_file, natural_sort_key remain the same) ...\n",
        "def upload_to_gemini(path, mime_type=\"image/png\"):\n",
        "    \"\"\"Uploads a file to Gemini and returns the File object.\"\"\"\n",
        "    try:\n",
        "        file = genai.upload_file(path, mime_type=mime_type)\n",
        "        return file\n",
        "    except Exception as e:\n",
        "        print(f\"  🛑 Error uploading file {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def delete_gemini_file(file_object):\n",
        "    \"\"\"Deletes a file from Gemini service.\"\"\"\n",
        "    if file_object:\n",
        "        try: genai.delete_file(file_object.name)\n",
        "        except Exception as e: print(f\"  ⚠️ Warning: Failed to delete file {file_object.name}: {e}\")\n",
        "\n",
        "def natural_sort_key(s):\n",
        "    \"\"\"Key for sorting strings containing numbers naturally.\"\"\"\n",
        "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "def process_folder(folder_path):\n",
        "    if not os.path.isdir(folder_path):\n",
        "        print(f\"🛑 Error: Target folder not found at '{folder_path}'\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nStarting OCR process for folder: '{os.path.basename(folder_path)}'\")\n",
        "\n",
        "    # 1. Extract folder info\n",
        "    sujet_copie, nom_complet_sous_dossier = extract_folder_info(folder_path)\n",
        "    if not sujet_copie:\n",
        "        print(f\"🛑 Error: Couldn't extract info from folder path '{folder_path}'\")\n",
        "        return\n",
        "\n",
        "    print(f\"  Subject detected: '{sujet_copie}'\")\n",
        "\n",
        "    # Generate output path\n",
        "    parent_dir = os.path.dirname(folder_path)\n",
        "    sanitized_full_name = sanitize_filename(nom_complet_sous_dossier)\n",
        "    OUTPUT_CSV_FILE = os.path.join(parent_dir, f\"{sanitized_full_name}.csv\")\n",
        "    print(f\"  Output CSV will be: '{OUTPUT_CSV_FILE}'\")\n",
        "\n",
        "    try:\n",
        "        with open(OUTPUT_CSV_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([nom_complet_sous_dossier])\n",
        "\n",
        "            # Find and sort images\n",
        "            try:\n",
        "                all_files = [f for f in os.listdir(folder_path) if f.lower().endswith(\".png\")]\n",
        "                sorted_chunks = sorted(all_files, key=natural_sort_key)\n",
        "            except OSError as e:\n",
        "                print(f\"  🛑 Error listing files: {e}\")\n",
        "                sorted_chunks = []\n",
        "\n",
        "            if not sorted_chunks:\n",
        "                print(\"  No PNG files found.\")\n",
        "                return\n",
        "\n",
        "            print(f\"  Found {len(sorted_chunks)} chunks to process.\")\n",
        "            ocr_result_n_minus_1 = \"\"\n",
        "            ocr_result_n_minus_2 = \"\"\n",
        "\n",
        "            for i, chunk_filename in enumerate(tqdm(sorted_chunks, desc=\"Processing Chunks\")):\n",
        "                chunk_path = os.path.join(folder_path, chunk_filename)\n",
        "                gemini_file = None\n",
        "                current_ocr_result = \"\"\n",
        "\n",
        "                try:\n",
        "                    # System prompt selection\n",
        "                    if i == 0:\n",
        "                        system_instruction = SYSTEM_INSTRUCTION_FIRST_CHUNK.format(sujet_copie=sujet_copie)\n",
        "                        prompt_parts = []\n",
        "                    else:\n",
        "                        system_instruction = SYSTEM_INSTRUCTION_WITH_CONTEXT.format(sujet_copie=sujet_copie)\n",
        "                        context_text = \"\\n--- Texte Précédent ---\\n\"\n",
        "                        if ocr_result_n_minus_2:\n",
        "                            context_text += ocr_result_n_minus_2 + \"\\n\"\n",
        "                        if ocr_result_n_minus_1:\n",
        "                            context_text += ocr_result_n_minus_1\n",
        "                        context_text += \"\\n--- Fin du Texte Précédent ---\\n\\n--- Image Actuelle à Transcrire ---\"\n",
        "                        prompt_parts = [context_text]\n",
        "\n",
        "                    # Upload and process image\n",
        "                    gemini_file = upload_to_gemini(chunk_path)\n",
        "                    if not gemini_file:\n",
        "                        current_ocr_result = f\"ERROR: Upload failed {chunk_filename}\"\n",
        "                        writer.writerow([current_ocr_result])\n",
        "                        continue\n",
        "\n",
        "                    prompt_parts.append(gemini_file)\n",
        "                    model = genai.GenerativeModel(\n",
        "                        model_name=MODEL_NAME,\n",
        "                        generation_config=GENERATION_CONFIG,\n",
        "                        system_instruction=system_instruction,\n",
        "                    )\n",
        "\n",
        "                    # API call\n",
        "                    response = model.generate_content(prompt_parts, request_options={'timeout': 600})\n",
        "\n",
        "                    if response and response.parts:\n",
        "                        current_ocr_result = response.text.strip()\n",
        "                    else:\n",
        "                        current_ocr_result = \"ERROR: Empty response\"\n",
        "\n",
        "                    writer.writerow([current_ocr_result])\n",
        "\n",
        "                    # Update history\n",
        "                    ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                    ocr_result_n_minus_1 = current_ocr_result\n",
        "\n",
        "                except Exception as e_api:\n",
        "                    print(f\"🛑 Error processing {chunk_filename}:\")\n",
        "                    traceback.print_exc()\n",
        "                    writer.writerow([f\"ERROR: {type(e_api).__name__}\"])\n",
        "\n",
        "                finally:\n",
        "                    if gemini_file:\n",
        "                        delete_gemini_file(gemini_file)\n",
        "                    time.sleep(1.5)\n",
        "\n",
        "    except IOError as e:\n",
        "        print(f\"🛑 CSV Error: {e}\")\n",
        "    except Exception as e_main:\n",
        "        print(f\"🛑 Unexpected error:\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(f\"\\nCompleted processing for: {os.path.basename(folder_path)}\")\n",
        "    print(f\"CSV saved at: {OUTPUT_CSV_FILE}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.isdir(MAIN_FOLDER_PATH):\n",
        "        print(f\"🛑 Main folder not found: {MAIN_FOLDER_PATH}\")\n",
        "    else:\n",
        "        print(f\"\\nStarting batch processing for: {MAIN_FOLDER_PATH}\")\n",
        "        subfolders = [os.path.join(MAIN_FOLDER_PATH, d)\n",
        "                     for d in os.listdir(MAIN_FOLDER_PATH)\n",
        "                     if os.path.isdir(os.path.join(MAIN_FOLDER_PATH, d))]\n",
        "\n",
        "        subfolders.sort(key=natural_sort_key)  # Natural sort subfolders\n",
        "\n",
        "        print(f\"Found {len(subfolders)} subfolder(s) to process\")\n",
        "        for idx, subfolder in enumerate(subfolders, 1):\n",
        "            print(f\"\\n=== Processing subfolder {idx}/{len(subfolders)}: {os.path.basename(subfolder)} ===\")\n",
        "            process_folder(subfolder)\n",
        "\n",
        "        print(\"\\nBatch processing completed for all subfolders!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773,
          "referenced_widgets": [
            "80059af4ffca4ebaaad819abff66daa6",
            "b52a62464491411088075b1ed54de551",
            "b2b76ed0dd254f69b298001dc4decbca",
            "bbc5e49b3114456e92c1dbabb9db0bac",
            "3fbf65add74a41019f8c1ee0fa0f8960",
            "b47d45a2fc62403d88b3c3fb45ff25d1",
            "8746f79639ce4915afcfdb57f7f41896",
            "906b0651be4549a5830cfbcd932fa96c",
            "b20e3ce6c0e44f16a28039fa03ecd0bf",
            "95ae33f666b74a37bff42db73e43787b",
            "5873a05d94ce47589b19de739a9b44cf",
            "fb80e83ef7e648c6b61eb499d904c98b",
            "d7d2c6ddd6464b91a083af755ddc36f1",
            "8ba1ab979aa74dfa8e4afc02a8571c32",
            "3421eb6875d94aad83a970632501e01d",
            "56a97efb0a424de8af40320125d99275",
            "7858b0458a8b4f0fbdc1a0e8cd61264a",
            "6b800820ca5f418c99ea3ac9656e1bdb",
            "ba1c0a00440d40e7b55c12c9b6abeb97",
            "57b9a98902424e3e9142dfc2000bd7ec",
            "1be6f25bf4204073bef3bce784f3be86",
            "a65b951301ad47ca9108644c28c55fe2",
            "c4d5ec62806b408e9455dc29437e6d64",
            "d27944631693451cb082ef01749e3c60",
            "7761f1906d9b4e0dbc186f34d4f45d0a",
            "6fc1031ecb1c4235a6577b5b39341d5d",
            "342b8b884aff42738dd2be52a4d0b40b",
            "b6584c81f7484b859e116790252a0114",
            "b4933d16168846e88cc8ccbfd7b929f0",
            "3f87a4847bb3445db0c51d3df64e20b2",
            "97200a866ebb4ed2ada45e9fd25c0c9e",
            "14fb29bb808f42d08ef3878de5063bfd",
            "c93332724b8f41bc9b46e578e3d829dd"
          ]
        },
        "cellView": "form",
        "id": "BCAwf1IEqi5U",
        "outputId": "192d83c6-90d8-4cca-9287-c2f2b5025fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🛑 Error: GEMINI_API_KEY environment variable not set.\n",
            "\n",
            "Starting batch processing for: /content/main_folder\n",
            "Found 21 subfolder(s) to process\n",
            "\n",
            "=== Processing subfolder 1/21: .ipynb_checkpoints ===\n",
            "\n",
            "Starting OCR process for folder: '.ipynb_checkpoints'\n",
            "  Subject detected: '.ipynb_checkpoints'\n",
            "  Output CSV will be: '/content/main_folder/.ipynb_checkpoints.csv'\n",
            "  No PNG files found.\n",
            "\n",
            "=== Processing subfolder 2/21: Commentaire de Diderot (AGREG ext. - note _ 10) ===\n",
            "\n",
            "Starting OCR process for folder: 'Commentaire de Diderot (AGREG ext. - note _ 10)'\n",
            "  Subject detected: 'Commentaire de Diderot (AGREG ext. - note _ 10)'\n",
            "  Output CSV will be: '/content/main_folder/Commentaire de Diderot (AGREG ext. - note _ 10).csv'\n",
            "  Found 42 chunks to process.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Chunks:   0%|          | 0/42 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80059af4ffca4ebaaad819abff66daa6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Completed processing for: Commentaire de Diderot (AGREG ext. - note _ 10)\n",
            "CSV saved at: /content/main_folder/Commentaire de Diderot (AGREG ext. - note _ 10).csv\n",
            "\n",
            "=== Processing subfolder 3/21: Commentaire de Diderot (CAPES 2019 - note _ 19) ===\n",
            "\n",
            "Starting OCR process for folder: 'Commentaire de Diderot (CAPES 2019 - note _ 19)'\n",
            "  Subject detected: 'Commentaire de Diderot (CAPES 2019 - note _ 19)'\n",
            "  Output CSV will be: '/content/main_folder/Commentaire de Diderot (CAPES 2019 - note _ 19).csv'\n",
            "  Found 34 chunks to process.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Chunks:   0%|          | 0/34 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb80e83ef7e648c6b61eb499d904c98b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Completed processing for: Commentaire de Diderot (CAPES 2019 - note _ 19)\n",
            "CSV saved at: /content/main_folder/Commentaire de Diderot (CAPES 2019 - note _ 19).csv\n",
            "\n",
            "=== Processing subfolder 4/21: Commentaire de Foucault - Naissance de la clinique (AGREG interne 2021 - note 13) ===\n",
            "\n",
            "Starting OCR process for folder: 'Commentaire de Foucault - Naissance de la clinique (AGREG interne 2021 - note 13)'\n",
            "  Subject detected: 'Commentaire de Foucault - Naissance de la clinique (AGREG interne 2021 - note 13)'\n",
            "  Output CSV will be: '/content/main_folder/Commentaire de Foucault - Naissance de la clinique (AGREG interne 2021 - note 13).csv'\n",
            "  Found 39 chunks to process.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Chunks:   0%|          | 0/39 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4d5ec62806b408e9455dc29437e6d64"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CSV File mover\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "def move_csv_files(source_dir, target_dir):\n",
        "    # Create the target directory if it doesn't exist\n",
        "    if not os.path.exists(target_dir):\n",
        "        os.makedirs(target_dir)\n",
        "        print(f\"Created directory: {target_dir}\")\n",
        "\n",
        "    # Find all CSV files in the source directory\n",
        "    csv_pattern = os.path.join(source_dir, \"*.csv\")\n",
        "    csv_files = glob.glob(csv_pattern)\n",
        "\n",
        "    # Also search in subdirectories\n",
        "    for root, dirs, files in os.walk(source_dir):\n",
        "        for file in files:\n",
        "            if file.lower().endswith('.csv'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                if file_path not in csv_files:  # Avoid duplicates\n",
        "                    csv_files.append(file_path)\n",
        "\n",
        "    # Move each CSV file to the target directory\n",
        "    moved_count = 0\n",
        "    for file_path in csv_files:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        destination = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Handle file name conflicts\n",
        "        if os.path.exists(destination):\n",
        "            base, extension = os.path.splitext(file_name)\n",
        "            counter = 1\n",
        "            while os.path.exists(destination):\n",
        "                new_name = f\"{base}_{counter}{extension}\"\n",
        "                destination = os.path.join(target_dir, new_name)\n",
        "                counter += 1\n",
        "\n",
        "        # Move the file\n",
        "        shutil.move(file_path, destination)\n",
        "        moved_count += 1\n",
        "        print(f\"Moved: {file_path} → {destination}\")\n",
        "\n",
        "    return moved_count\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    source_directory = \"/content/main_folder\"\n",
        "    target_directory = \"/content/CSV\"\n",
        "\n",
        "    print(f\"Searching for CSV files in {source_directory}...\")\n",
        "    moved = move_csv_files(source_directory, target_directory)\n",
        "    print(f\"Successfully moved {moved} CSV file(s) to {target_directory}\")"
      ],
      "metadata": {
        "id": "ITjILJyGter0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src = '/content/drive/MyDrive/folder_partitioned_merged/Le bonheur est-il affaire de hasard ou de nécessité ? (AGREG interne 2022 - note : 14)'\n",
        "dst = '/content/Le bonheur est-il affaire de hasard ou de nécessité 14 ? (AGREG interne 2022 - note : 14)'\n",
        "\n",
        "if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "shutil.copytree(src, dst)"
      ],
      "metadata": {
        "id": "VEMlERMSpQPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OCR avec enrengistrement en CSV au même nom\n",
        "\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import csv\n",
        "import time\n",
        "import re # For natural sorting and filename sanitization\n",
        "from tqdm.notebook import tqdm # Progress bars\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# --- Configuration ---\n",
        "TARGET_FOLDER_PATH = \"/content/folder\"\n",
        "# Output CSV file path will be generated dynamically later\n",
        "\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "    print(\"Gemini API Key configured.\")\n",
        "except KeyError:\n",
        "    print(\"🛑 Error: GEMINI_API_KEY environment variable not set.\")\n",
        "    # sys.exit(1) # Optional: exit if running unattended\n",
        "except Exception as e:\n",
        "    print(f\"🛑 Error configuring Gemini API: {e}\")\n",
        "    # sys.exit(1) # Optional: exit if running unattended\n",
        "\n",
        "\n",
        "# Gemini Model Configuration)\n",
        "GENERATION_CONFIG = {\n",
        "  \"temperature\": 0.2,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "MODEL_NAME = \"gemini-2.5-pro-preview-03-25\"\n",
        "\n",
        "\n",
        "# System Prompts (remain the same)\n",
        "SYSTEM_INSTRUCTION_WITH_CONTEXT = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\".\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\n",
        "Aide Contextuelle (Texte Précédent) : Pour t'aider à déchiffrer l'écriture, le texte qui précède *immédiatement* celui de l'image actuelle est fourni ci-dessous (séparé par '--- Texte Précédent ---'). Ce texte a été obtenu par OCR des sections précédentes de la dissertation. Le texte de l'image actuelle commence là où le texte précédent s'arrête (il peut y avoir un léger chevauchement de lignes dû au découpage des images).\n",
        "\n",
        "**Priorité :** Ta tâche principale reste de transcrire *exactement* ce que tu vois sur l'image actuelle. Utilise le texte précédent **uniquement** comme une aide *secondaire* pour résoudre des ambiguïtés ou déchiffrer des mots très difficiles. **Ne laisse PAS le texte précédent remplacer ou modifier ce qui est clairement visible sur l'image actuelle.** En cas de conflit entre le texte précédent et l'image, **la transcription fidèle de l'image PRÉVAUT.**\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_INSTRUCTION_FIRST_CHUNK = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\". Ceci est le tout début de la dissertation.\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\"\"\"\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def extract_folder_info(folder_path):\n",
        "    \"\"\"Extracts the subject and full name from a folder path name.\"\"\"\n",
        "    if not os.path.isdir(folder_path): return None, None\n",
        "    full_name = os.path.basename(folder_path)\n",
        "    subject = None\n",
        "    index_parenthese = full_name.find('(')\n",
        "    if index_parenthese != -1: subject = full_name[:index_parenthese].strip()\n",
        "    else: subject = full_name.strip() # Fallback\n",
        "    return subject, full_name\n",
        "\n",
        "# --- NEW: Function to sanitize filename ---\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Removes or replaces characters invalid for filenames.\"\"\"\n",
        "    # Remove leading/trailing whitespace\n",
        "    sanitized = filename.strip()\n",
        "    # Replace common invalid characters with underscore (adjust as needed)\n",
        "    sanitized = re.sub(r'[\\\\/*?:\"<>|]', '_', sanitized)\n",
        "    # Replace forward slash specifically (common in paths)\n",
        "    sanitized = sanitized.replace('/', '_')\n",
        "    # Optional: Limit length if necessary (e.g., 200 chars)\n",
        "    # max_len = 200\n",
        "    # if len(sanitized) > max_len:\n",
        "    #     name, ext = os.path.splitext(sanitized)\n",
        "    #     sanitized = name[:max_len - len(ext) - 1] + '~' + ext\n",
        "    return sanitized\n",
        "\n",
        "\n",
        "# ... (upload_to_gemini, delete_gemini_file, natural_sort_key remain the same) ...\n",
        "def upload_to_gemini(path, mime_type=\"image/png\"):\n",
        "    \"\"\"Uploads a file to Gemini and returns the File object.\"\"\"\n",
        "    try:\n",
        "        file = genai.upload_file(path, mime_type=mime_type)\n",
        "        return file\n",
        "    except Exception as e:\n",
        "        print(f\"  🛑 Error uploading file {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def delete_gemini_file(file_object):\n",
        "    \"\"\"Deletes a file from Gemini service.\"\"\"\n",
        "    if file_object:\n",
        "        try: genai.delete_file(file_object.name)\n",
        "        except Exception as e: print(f\"  ⚠️ Warning: Failed to delete file {file_object.name}: {e}\")\n",
        "\n",
        "def natural_sort_key(s):\n",
        "    \"\"\"Key for sorting strings containing numbers naturally.\"\"\"\n",
        "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
        "\n",
        "\n",
        "# --- Main Processing Logic for Single Folder ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.isdir(TARGET_FOLDER_PATH):\n",
        "        print(f\"🛑 Error: Target folder not found at '{TARGET_FOLDER_PATH}'\")\n",
        "    else:\n",
        "        print(f\"Starting OCR process for single folder: '{os.path.basename(TARGET_FOLDER_PATH)}'\")\n",
        "\n",
        "        # 1. Extract Subject and Full Name for the target folder\n",
        "        sujet_copie, nom_complet_sous_dossier = extract_folder_info(TARGET_FOLDER_PATH)\n",
        "\n",
        "        if sujet_copie is None:\n",
        "            print(f\"🛑 Error: Could not extract info from folder path '{TARGET_FOLDER_PATH}'. Cannot proceed.\")\n",
        "        else:\n",
        "            print(f\"  Subject detected: '{sujet_copie}'\")\n",
        "\n",
        "            # --- GENERATE DYNAMIC OUTPUT CSV FILENAME ---\n",
        "            parent_dir = os.path.dirname(TARGET_FOLDER_PATH) # Get directory containing the target folder\n",
        "            sanitized_subject = sanitize_filename(sujet_copie) # Sanitize the subject for use in filename\n",
        "            OUTPUT_CSV_FILE = os.path.join(parent_dir, f\"{sanitized_subject}.csv\")\n",
        "            print(f\"  Output CSV will be saved as: '{OUTPUT_CSV_FILE}'\")\n",
        "            # --- End Filename Generation ---\n",
        "\n",
        "            # --- Initialize CSV Writer ---\n",
        "            try:\n",
        "                with open(OUTPUT_CSV_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                    writer = csv.writer(csvfile)\n",
        "                    print(\"  CSV file opened successfully.\")\n",
        "\n",
        "                    # Write the folder name (title) to the CSV\n",
        "                    writer.writerow([nom_complet_sous_dossier])\n",
        "                    print(f\"  Wrote folder title to CSV: {nom_complet_sous_dossier}\")\n",
        "\n",
        "                    # 2. Find and Sort Image Chunks\n",
        "                    # ... (Chunk finding logic remains the same) ...\n",
        "                    try:\n",
        "                        all_files = [f for f in os.listdir(TARGET_FOLDER_PATH) if f.lower().endswith(\".png\")]\n",
        "                        sorted_chunks = sorted(all_files, key=natural_sort_key)\n",
        "                    except OSError as e:\n",
        "                        print(f\"  🛑 Error listing files in {TARGET_FOLDER_PATH}: {e}\")\n",
        "                        sorted_chunks = []\n",
        "\n",
        "\n",
        "                    if not sorted_chunks:\n",
        "                        print(\"  No PNG chunk files found in this folder.\")\n",
        "                    else:\n",
        "                        print(f\"  Found {len(sorted_chunks)} chunk(s) to process.\")\n",
        "\n",
        "                        # --- Variables to store previous OCR results ---\n",
        "                        ocr_result_n_minus_1 = \"\"\n",
        "                        ocr_result_n_minus_2 = \"\"\n",
        "\n",
        "                        # --- Loop through Image Chunks ---\n",
        "                        # ... (The rest of the loop logic remains exactly the same as the previous version) ...\n",
        "                        for i, chunk_filename in enumerate(tqdm(sorted_chunks, desc=\"Processing Chunks\")):\n",
        "                            chunk_path = os.path.join(TARGET_FOLDER_PATH, chunk_filename)\n",
        "                            gemini_file = None\n",
        "                            current_ocr_result = \"\" # Initialize for this chunk\n",
        "\n",
        "                            print(f\"\\n    Processing chunk {i+1}/{len(sorted_chunks)}: {chunk_filename}\")\n",
        "\n",
        "                            try:\n",
        "                                # 3. Select System Prompt and Prepare Context\n",
        "                                prompt_parts = []\n",
        "                                if i == 0:\n",
        "                                    # First chunk: No preceding context\n",
        "                                    current_system_instruction = SYSTEM_INSTRUCTION_FIRST_CHUNK.format(sujet_copie=sujet_copie)\n",
        "                                    print(\"      Using FIRST chunk system prompt.\")\n",
        "                                else:\n",
        "                                    # Subsequent chunks: Prepare context text\n",
        "                                    current_system_instruction = SYSTEM_INSTRUCTION_WITH_CONTEXT.format(sujet_copie=sujet_copie)\n",
        "                                    context_text = \"\\n--- Texte Précédent ---\\n\"\n",
        "                                    if ocr_result_n_minus_2: # Add N-2 if available\n",
        "                                        context_text += ocr_result_n_minus_2 + \"\\n\" # Add newline separator\n",
        "                                    if ocr_result_n_minus_1: # Add N-1 if available\n",
        "                                        context_text += ocr_result_n_minus_1\n",
        "                                    context_text += \"\\n--- Fin du Texte Précédent ---\\n\\n--- Image Actuelle à Transcrire ---\"\n",
        "                                    # Add context text as the first part of the prompt\n",
        "                                    prompt_parts.append(context_text)\n",
        "                                    print(f\"      Using CONTEXT system prompt (Context length: ~{len(context_text)} chars).\")\n",
        "\n",
        "\n",
        "                                # 4. Upload Image Chunk\n",
        "                                print(f\"      Uploading: {chunk_filename}...\")\n",
        "                                gemini_file = upload_to_gemini(chunk_path)\n",
        "                                if gemini_file is None:\n",
        "                                    current_ocr_result = f\"ERROR: Upload failed for {chunk_filename}\"\n",
        "                                    writer.writerow([current_ocr_result])\n",
        "                                    print(f\"      {current_ocr_result}\")\n",
        "                                    continue # Skip to next chunk\n",
        "\n",
        "                                # Add the image file as the next part of the prompt\n",
        "                                prompt_parts.append(gemini_file)\n",
        "\n",
        "                                # 5. Initialize Model (Can potentially be outside loop if system prompt doesn't change often, but safer inside for now)\n",
        "                                # Re-create model instance ensures the correct system prompt is used\n",
        "                                model = genai.GenerativeModel(\n",
        "                                    model_name=MODEL_NAME,\n",
        "                                    generation_config=GENERATION_CONFIG,\n",
        "                                    system_instruction=current_system_instruction,\n",
        "                                )\n",
        "\n",
        "                                # 6. Call Gemini API\n",
        "                                print(\"      Calling Gemini API...\")\n",
        "                                response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
        "\n",
        "                                # 7. Extract OCR Result\n",
        "                                if response and response.parts:\n",
        "                                     try:\n",
        "                                         current_ocr_result = response.text\n",
        "                                         # Optional: Remove potential leading/trailing whitespace\n",
        "                                         current_ocr_result = current_ocr_result.strip()\n",
        "                                         print(f\"      OCR result received (~{len(current_ocr_result)} chars).\")\n",
        "                                     except ValueError:\n",
        "                                         print(f\"      ⚠️ Warning: Could not extract text from response for {chunk_filename}. Response: {response.prompt_feedback}\")\n",
        "                                         current_ocr_result = f\"ERROR: No text found or content blocked ({response.prompt_feedback})\"\n",
        "                                     except Exception as e_resp:\n",
        "                                        print(f\"      🛑 Error extracting text from response: {e_resp}\")\n",
        "                                        current_ocr_result = f\"ERROR: Response parsing failed - {e_resp}\"\n",
        "                                else:\n",
        "                                    print(f\"      ⚠️ Warning: Empty or invalid response received for {chunk_filename}.\")\n",
        "                                    current_ocr_result = \"ERROR: Empty or invalid response from API\"\n",
        "\n",
        "                                # Write result to CSV\n",
        "                                writer.writerow([current_ocr_result])\n",
        "                                # print(f\"      Wrote OCR result to CSV.\")\n",
        "\n",
        "                                # 8. Update History for Next Iteration\n",
        "                                ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                                ocr_result_n_minus_1 = current_ocr_result # Store the raw result from this chunk\n",
        "\n",
        "                            except Exception as e_api:\n",
        "                                print(f\"    🛑 An error occurred during API call or processing for {chunk_filename}:\")\n",
        "                                traceback.print_exc() # Print full traceback for debugging\n",
        "                                error_message = f\"ERROR: API call failed for {chunk_filename} - {type(e_api).__name__}\"\n",
        "                                writer.writerow([error_message])\n",
        "                                # Update history with error marker to avoid propagating bad context\n",
        "                                ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                                ocr_result_n_minus_1 = error_message # Store error marker\n",
        "\n",
        "                            finally:\n",
        "                                # 9. Delete Uploaded File from Gemini\n",
        "                                if gemini_file:\n",
        "                                    delete_gemini_file(gemini_file)\n",
        "                                time.sleep(1.5) # Slightly increased delay\n",
        "\n",
        "\n",
        "            except IOError as e:\n",
        "                print(f\"🛑 Error opening or writing to CSV file {OUTPUT_CSV_FILE}: {e}\")\n",
        "            except Exception as e_main:\n",
        "                 print(f\"🛑 An unexpected error occurred during the main process:\")\n",
        "                 traceback.print_exc()\n",
        "\n",
        "            print(\"\\n--- OCR Process Complete for Folder ---\")\n",
        "            print(f\"Results saved in: {OUTPUT_CSV_FILE}\") # Display the final generated path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7fa8daadea2b47c5bcdfb442cfc2e7a6",
            "3ab72cc74d7e4676920c8d5962c0e6db",
            "f0705ec542e649bfa26bb4b007105a3d",
            "a1a58928ba7c4a1fbdbc7797b1ce998e",
            "11f6a0bf1b004efcb68ac9c4302a9ba2",
            "76b996a4c076474e89fcb949113fb8ad",
            "fe4b1bbb0e854dfabdf13f3e643e6388",
            "338f2eea4c8e4a5fb07452ef340047cb",
            "819c99a985974144b5eb5a94cfa36471",
            "c4f5ba1d21a34604ac502272466e2026",
            "4c3d4dd166754e2e8fa292aa195b192c"
          ]
        },
        "cellView": "form",
        "id": "U77kRkzJo4-8",
        "outputId": "479fcc75-2c9a-493f-cf92-4d09f82fc11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API Key configured.\n",
            "Starting OCR process for single folder: 'Le bonheur est-il affaire de hasard et de nécessité 15.5 ? (AGREG interne -  note : 15,5)'\n",
            "  Subject detected: 'Le bonheur est-il affaire de hasard et de nécessité 15.5 ?'\n",
            "  Output CSV will be saved as: '/content/Le bonheur est-il affaire de hasard et de nécessité 15.5 _.csv'\n",
            "  CSV file opened successfully.\n",
            "  Wrote folder title to CSV: Le bonheur est-il affaire de hasard et de nécessité 15.5 ? (AGREG interne -  note : 15,5)\n",
            "  Found 51 chunk(s) to process.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Chunks:   0%|          | 0/51 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7fa8daadea2b47c5bcdfb442cfc2e7a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Processing chunk 1/51: 0001.png\n",
            "      Using FIRST chunk system prompt.\n",
            "      Uploading: 0001.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~719 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/ars31p8iz8mm: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "    Processing chunk 2/51: 0002.png\n",
            "      Using CONTEXT system prompt (Context length: ~812 chars).\n",
            "      Uploading: 0002.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~651 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/7zacz5knf2rr: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "    Processing chunk 3/51: 0003.png\n",
            "      Using CONTEXT system prompt (Context length: ~1464 chars).\n",
            "      Uploading: 0003.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~311 chars).\n",
            "\n",
            "    Processing chunk 4/51: 0004.png\n",
            "      Using CONTEXT system prompt (Context length: ~1056 chars).\n",
            "      Uploading: 0004.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~666 chars).\n",
            "\n",
            "    Processing chunk 5/51: 0005.png\n",
            "      Using CONTEXT system prompt (Context length: ~1071 chars).\n",
            "      Uploading: 0005.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~645 chars).\n",
            "\n",
            "    Processing chunk 6/51: 0006.png\n",
            "      Using CONTEXT system prompt (Context length: ~1405 chars).\n",
            "      Uploading: 0006.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~476 chars).\n",
            "\n",
            "    Processing chunk 7/51: 0007.png\n",
            "      Using CONTEXT system prompt (Context length: ~1215 chars).\n",
            "      Uploading: 0007.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~447 chars).\n",
            "\n",
            "    Processing chunk 8/51: 0008.png\n",
            "      Using CONTEXT system prompt (Context length: ~1017 chars).\n",
            "      Uploading: 0008.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~632 chars).\n",
            "\n",
            "    Processing chunk 9/51: 0009.png\n",
            "      Using CONTEXT system prompt (Context length: ~1173 chars).\n",
            "      Uploading: 0009.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~631 chars).\n",
            "\n",
            "    Processing chunk 10/51: 0010.png\n",
            "      Using CONTEXT system prompt (Context length: ~1357 chars).\n",
            "      Uploading: 0010.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~262 chars).\n",
            "\n",
            "    Processing chunk 11/51: 0011.png\n",
            "      Using CONTEXT system prompt (Context length: ~987 chars).\n",
            "      Uploading: 0011.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~593 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/daef2bt93qdw: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 12/51: 0012.png\n",
            "      Using CONTEXT system prompt (Context length: ~949 chars).\n",
            "      Uploading: 0012.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~650 chars).\n",
            "\n",
            "    Processing chunk 13/51: 0013.png\n",
            "      Using CONTEXT system prompt (Context length: ~1337 chars).\n",
            "      Uploading: 0013.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~617 chars).\n",
            "\n",
            "    Processing chunk 14/51: 0014.png\n",
            "      Using CONTEXT system prompt (Context length: ~1361 chars).\n",
            "      Uploading: 0014.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~245 chars).\n",
            "\n",
            "    Processing chunk 15/51: 0015.png\n",
            "      Using CONTEXT system prompt (Context length: ~956 chars).\n",
            "      Uploading: 0015.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~608 chars).\n",
            "\n",
            "    Processing chunk 16/51: 0016.png\n",
            "      Using CONTEXT system prompt (Context length: ~947 chars).\n",
            "      Uploading: 0016.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~537 chars).\n",
            "\n",
            "    Processing chunk 17/51: 0017.png\n",
            "      Using CONTEXT system prompt (Context length: ~1239 chars).\n",
            "      Uploading: 0017.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~226 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/p6zf60vy63dk: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 18/51: 0018.png\n",
            "      Using CONTEXT system prompt (Context length: ~857 chars).\n",
            "      Uploading: 0018.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~661 chars).\n",
            "\n",
            "    Processing chunk 19/51: 0019.png\n",
            "      Using CONTEXT system prompt (Context length: ~981 chars).\n",
            "      Uploading: 0019.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~530 chars).\n",
            "\n",
            "    Processing chunk 20/51: 0020.png\n",
            "      Using CONTEXT system prompt (Context length: ~1285 chars).\n",
            "      Uploading: 0020.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~496 chars).\n",
            "\n",
            "    Processing chunk 21/51: 0021.png\n",
            "      Using CONTEXT system prompt (Context length: ~1120 chars).\n",
            "      Uploading: 0021.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~543 chars).\n",
            "\n",
            "    Processing chunk 22/51: 0022.png\n",
            "      Using CONTEXT system prompt (Context length: ~1133 chars).\n",
            "      Uploading: 0022.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~605 chars).\n",
            "\n",
            "    Processing chunk 23/51: 0023.png\n",
            "      Using CONTEXT system prompt (Context length: ~1242 chars).\n",
            "      Uploading: 0023.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~608 chars).\n",
            "\n",
            "    Processing chunk 24/51: 0024.png\n",
            "      Using CONTEXT system prompt (Context length: ~1307 chars).\n",
            "      Uploading: 0024.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~246 chars).\n",
            "\n",
            "    Processing chunk 25/51: 0025.png\n",
            "      Using CONTEXT system prompt (Context length: ~948 chars).\n",
            "      Uploading: 0025.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~603 chars).\n",
            "\n",
            "    Processing chunk 26/51: 0026.png\n",
            "      Using CONTEXT system prompt (Context length: ~943 chars).\n",
            "      Uploading: 0026.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~574 chars).\n",
            "\n",
            "    Processing chunk 27/51: 0027.png\n",
            "      Using CONTEXT system prompt (Context length: ~1271 chars).\n",
            "      Uploading: 0027.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~597 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/yas3qr8bl2jd: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "    Processing chunk 28/51: 0028.png\n",
            "      Using CONTEXT system prompt (Context length: ~1265 chars).\n",
            "      Uploading: 0028.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~188 chars).\n",
            "\n",
            "    Processing chunk 29/51: 0029.png\n",
            "      Using CONTEXT system prompt (Context length: ~879 chars).\n",
            "      Uploading: 0029.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~593 chars).\n",
            "\n",
            "    Processing chunk 30/51: 0030.png\n",
            "      Using CONTEXT system prompt (Context length: ~875 chars).\n",
            "      Uploading: 0030.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~652 chars).\n",
            "\n",
            "    Processing chunk 31/51: 0031.png\n",
            "      Using CONTEXT system prompt (Context length: ~1339 chars).\n",
            "      Uploading: 0031.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~269 chars).\n",
            "\n",
            "    Processing chunk 32/51: 0032.png\n",
            "      Using CONTEXT system prompt (Context length: ~1015 chars).\n",
            "      Uploading: 0032.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~642 chars).\n",
            "\n",
            "    Processing chunk 33/51: 0033.png\n",
            "      Using CONTEXT system prompt (Context length: ~1005 chars).\n",
            "      Uploading: 0033.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~626 chars).\n",
            "\n",
            "    Processing chunk 34/51: 0034.png\n",
            "      Using CONTEXT system prompt (Context length: ~1362 chars).\n",
            "      Uploading: 0034.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~506 chars).\n",
            "\n",
            "    Processing chunk 35/51: 0035.png\n",
            "      Using CONTEXT system prompt (Context length: ~1226 chars).\n",
            "      Uploading: 0035.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~575 chars).\n",
            "\n",
            "    Processing chunk 36/51: 0036.png\n",
            "      Using CONTEXT system prompt (Context length: ~1175 chars).\n",
            "      Uploading: 0036.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~652 chars).\n",
            "\n",
            "    Processing chunk 37/51: 0037.png\n",
            "      Using CONTEXT system prompt (Context length: ~1321 chars).\n",
            "      Uploading: 0037.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~560 chars).\n",
            "\n",
            "    Processing chunk 38/51: 0038.png\n",
            "      Using CONTEXT system prompt (Context length: ~1306 chars).\n",
            "      Uploading: 0038.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~248 chars).\n",
            "\n",
            "    Processing chunk 39/51: 0039.png\n",
            "      Using CONTEXT system prompt (Context length: ~902 chars).\n",
            "      Uploading: 0039.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~481 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/hmhp0axldmbz: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 40/51: 0040.png\n",
            "      Using CONTEXT system prompt (Context length: ~823 chars).\n",
            "      Uploading: 0040.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~632 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/fn9a0cz5bv2e: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 41/51: 0041.png\n",
            "      Using CONTEXT system prompt (Context length: ~1207 chars).\n",
            "      Uploading: 0041.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~657 chars).\n",
            "\n",
            "    Processing chunk 42/51: 0042.png\n",
            "      Using CONTEXT system prompt (Context length: ~1383 chars).\n",
            "      Uploading: 0042.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~246 chars).\n",
            "\n",
            "    Processing chunk 43/51: 0043.png\n",
            "      Using CONTEXT system prompt (Context length: ~997 chars).\n",
            "      Uploading: 0043.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~612 chars).\n",
            "\n",
            "    Processing chunk 44/51: 0044.png\n",
            "      Using CONTEXT system prompt (Context length: ~952 chars).\n",
            "      Uploading: 0044.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~634 chars).\n",
            "\n",
            "    Processing chunk 45/51: 0045.png\n",
            "      Using CONTEXT system prompt (Context length: ~1340 chars).\n",
            "      Uploading: 0045.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~318 chars).\n",
            "\n",
            "    Processing chunk 46/51: 0046.png\n",
            "      Using CONTEXT system prompt (Context length: ~1046 chars).\n",
            "      Uploading: 0046.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~633 chars).\n",
            "\n",
            "    Processing chunk 47/51: 0047.png\n",
            "      Using CONTEXT system prompt (Context length: ~1045 chars).\n",
            "      Uploading: 0047.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~658 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/2aw48fwwn6lx: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 48/51: 0048.png\n",
            "      Using CONTEXT system prompt (Context length: ~1385 chars).\n",
            "      Uploading: 0048.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~542 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/ue6sj8dw9ucr: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "    Processing chunk 49/51: 0049.png\n",
            "      Using CONTEXT system prompt (Context length: ~1294 chars).\n",
            "      Uploading: 0049.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~520 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/pizpwhoocdc5: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 50/51: 0050.png\n",
            "      Using CONTEXT system prompt (Context length: ~1156 chars).\n",
            "      Uploading: 0050.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~499 chars).\n",
            "\n",
            "    Processing chunk 51/51: 0051.png\n",
            "      Using CONTEXT system prompt (Context length: ~1113 chars).\n",
            "      Uploading: 0051.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~232 chars).\n",
            "\n",
            "--- OCR Process Complete for Folder ---\n",
            "Results saved in: /content/Le bonheur est-il affaire de hasard et de nécessité 15.5 _.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDGh3kV4quO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OCR avec enrengistrement en CSV au même nom\n",
        "\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import csv\n",
        "import time\n",
        "import re # For natural sorting and filename sanitization\n",
        "from tqdm.notebook import tqdm # Progress bars\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# --- Configuration ---\n",
        "# *** SET THE TARGET FOLDER HERE ***\n",
        "TARGET_FOLDER_PATH = \"/content/Peut-on vivre en paix avec son inconscient ? (AGREG interne 2021 -note : 12)\" # Example: Set to the specific subfolder path you want to process\n",
        "\n",
        "# Output CSV file path will be generated dynamically later\n",
        "\n",
        "# Gemini API Key (ensure it's set as an environment variable)\n",
        "# ... (API Key configuration remains the same) ...\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "    print(\"Gemini API Key configured.\")\n",
        "except KeyError:\n",
        "    print(\"🛑 Error: GEMINI_API_KEY environment variable not set.\")\n",
        "    # sys.exit(1) # Optional: exit if running unattended\n",
        "except Exception as e:\n",
        "    print(f\"🛑 Error configuring Gemini API: {e}\")\n",
        "    # sys.exit(1) # Optional: exit if running unattended\n",
        "\n",
        "\n",
        "# Gemini Model Configuration)\n",
        "GENERATION_CONFIG = {\n",
        "  \"temperature\": 0.2, # Slightly lower temp might encourage more fidelity\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "MODEL_NAME = \"gemini-2.5-pro-preview-03-25\"\n",
        "\n",
        "\n",
        "# System Prompts (remain the same)\n",
        "SYSTEM_INSTRUCTION_WITH_CONTEXT = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\".\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\n",
        "Aide Contextuelle (Texte Précédent) : Pour t'aider à déchiffrer l'écriture, le texte qui précède *immédiatement* celui de l'image actuelle est fourni ci-dessous (séparé par '--- Texte Précédent ---'). Ce texte a été obtenu par OCR des sections précédentes de la dissertation. Le texte de l'image actuelle commence là où le texte précédent s'arrête (il peut y avoir un léger chevauchement de lignes dû au découpage des images).\n",
        "\n",
        "**Priorité Absolue :** Ta tâche principale reste de transcrire *exactement* ce que tu vois sur l'image actuelle. Utilise le texte précédent **uniquement** comme une aide *secondaire* pour résoudre des ambiguïtés ou déchiffrer des mots très difficiles. **Ne laisse PAS le texte précédent remplacer ou modifier ce qui est clairement visible sur l'image actuelle.** En cas de conflit entre le texte précédent et l'image, **la transcription fidèle de l'image PRÉVAUT.**\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_INSTRUCTION_FIRST_CHUNK = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\". Ceci est le tout début de la dissertation.\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\"\"\"\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def extract_folder_info(folder_path):\n",
        "    \"\"\"Extracts the subject and full name from a folder path name.\"\"\"\n",
        "    if not os.path.isdir(folder_path): return None, None\n",
        "    full_name = os.path.basename(folder_path)\n",
        "    subject = None\n",
        "    index_parenthese = full_name.find('(')\n",
        "    if index_parenthese != -1: subject = full_name[:index_parenthese].strip()\n",
        "    else: subject = full_name.strip() # Fallback\n",
        "    return subject, full_name\n",
        "\n",
        "# --- NEW: Function to sanitize filename ---\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Removes or replaces characters invalid for filenames.\"\"\"\n",
        "    # Remove leading/trailing whitespace\n",
        "    sanitized = filename.strip()\n",
        "    # Replace common invalid characters with underscore (adjust as needed)\n",
        "    sanitized = re.sub(r'[\\\\/*?:\"<>|]', '_', sanitized)\n",
        "    # Replace forward slash specifically (common in paths)\n",
        "    sanitized = sanitized.replace('/', '_')\n",
        "    # Optional: Limit length if necessary (e.g., 200 chars)\n",
        "    # max_len = 200\n",
        "    # if len(sanitized) > max_len:\n",
        "    #     name, ext = os.path.splitext(sanitized)\n",
        "    #     sanitized = name[:max_len - len(ext) - 1] + '~' + ext\n",
        "    return sanitized\n",
        "\n",
        "\n",
        "# ... (upload_to_gemini, delete_gemini_file, natural_sort_key remain the same) ...\n",
        "def upload_to_gemini(path, mime_type=\"image/png\"):\n",
        "    \"\"\"Uploads a file to Gemini and returns the File object.\"\"\"\n",
        "    try:\n",
        "        file = genai.upload_file(path, mime_type=mime_type)\n",
        "        return file\n",
        "    except Exception as e:\n",
        "        print(f\"  🛑 Error uploading file {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def delete_gemini_file(file_object):\n",
        "    \"\"\"Deletes a file from Gemini service.\"\"\"\n",
        "    if file_object:\n",
        "        try: genai.delete_file(file_object.name)\n",
        "        except Exception as e: print(f\"  ⚠️ Warning: Failed to delete file {file_object.name}: {e}\")\n",
        "\n",
        "def natural_sort_key(s):\n",
        "    \"\"\"Key for sorting strings containing numbers naturally.\"\"\"\n",
        "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
        "\n",
        "\n",
        "# --- Main Processing Logic for Single Folder ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.isdir(TARGET_FOLDER_PATH):\n",
        "        print(f\"🛑 Error: Target folder not found at '{TARGET_FOLDER_PATH}'\")\n",
        "    else:\n",
        "        print(f\"Starting OCR process for single folder: '{os.path.basename(TARGET_FOLDER_PATH)}'\")\n",
        "\n",
        "        # 1. Extract Subject and Full Name for the target folder\n",
        "        sujet_copie, nom_complet_sous_dossier = extract_folder_info(TARGET_FOLDER_PATH)\n",
        "\n",
        "        if sujet_copie is None:\n",
        "            print(f\"🛑 Error: Could not extract info from folder path '{TARGET_FOLDER_PATH}'. Cannot proceed.\")\n",
        "        else:\n",
        "            print(f\"  Subject detected: '{sujet_copie}'\")\n",
        "\n",
        "            # --- GENERATE DYNAMIC OUTPUT CSV FILENAME ---\n",
        "            parent_dir = os.path.dirname(TARGET_FOLDER_PATH) # Get directory containing the target folder\n",
        "            sanitized_subject = sanitize_filename(sujet_copie) # Sanitize the subject for use in filename\n",
        "            OUTPUT_CSV_FILE = os.path.join(parent_dir, f\"{sanitized_subject}.csv\")\n",
        "            print(f\"  Output CSV will be saved as: '{OUTPUT_CSV_FILE}'\")\n",
        "            # --- End Filename Generation ---\n",
        "\n",
        "            # --- Initialize CSV Writer ---\n",
        "            try:\n",
        "                with open(OUTPUT_CSV_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                    writer = csv.writer(csvfile)\n",
        "                    print(\"  CSV file opened successfully.\")\n",
        "\n",
        "                    # Write the folder name (title) to the CSV\n",
        "                    writer.writerow([nom_complet_sous_dossier])\n",
        "                    print(f\"  Wrote folder title to CSV: {nom_complet_sous_dossier}\")\n",
        "\n",
        "                    # 2. Find and Sort Image Chunks\n",
        "                    # ... (Chunk finding logic remains the same) ...\n",
        "                    try:\n",
        "                        all_files = [f for f in os.listdir(TARGET_FOLDER_PATH) if f.lower().endswith(\".png\")]\n",
        "                        sorted_chunks = sorted(all_files, key=natural_sort_key)\n",
        "                    except OSError as e:\n",
        "                        print(f\"  🛑 Error listing files in {TARGET_FOLDER_PATH}: {e}\")\n",
        "                        sorted_chunks = []\n",
        "\n",
        "\n",
        "                    if not sorted_chunks:\n",
        "                        print(\"  No PNG chunk files found in this folder.\")\n",
        "                    else:\n",
        "                        print(f\"  Found {len(sorted_chunks)} chunk(s) to process.\")\n",
        "\n",
        "                        # --- Variables to store previous OCR results ---\n",
        "                        ocr_result_n_minus_1 = \"\"\n",
        "                        ocr_result_n_minus_2 = \"\"\n",
        "\n",
        "                        # --- Loop through Image Chunks ---\n",
        "                        # ... (The rest of the loop logic remains exactly the same as the previous version) ...\n",
        "                        for i, chunk_filename in enumerate(tqdm(sorted_chunks, desc=\"Processing Chunks\")):\n",
        "                            chunk_path = os.path.join(TARGET_FOLDER_PATH, chunk_filename)\n",
        "                            gemini_file = None\n",
        "                            current_ocr_result = \"\" # Initialize for this chunk\n",
        "\n",
        "                            print(f\"\\n    Processing chunk {i+1}/{len(sorted_chunks)}: {chunk_filename}\")\n",
        "\n",
        "                            try:\n",
        "                                # 3. Select System Prompt and Prepare Context\n",
        "                                prompt_parts = []\n",
        "                                if i == 0:\n",
        "                                    # First chunk: No preceding context\n",
        "                                    current_system_instruction = SYSTEM_INSTRUCTION_FIRST_CHUNK.format(sujet_copie=sujet_copie)\n",
        "                                    print(\"      Using FIRST chunk system prompt.\")\n",
        "                                else:\n",
        "                                    # Subsequent chunks: Prepare context text\n",
        "                                    current_system_instruction = SYSTEM_INSTRUCTION_WITH_CONTEXT.format(sujet_copie=sujet_copie)\n",
        "                                    context_text = \"\\n--- Texte Précédent ---\\n\"\n",
        "                                    if ocr_result_n_minus_2: # Add N-2 if available\n",
        "                                        context_text += ocr_result_n_minus_2 + \"\\n\" # Add newline separator\n",
        "                                    if ocr_result_n_minus_1: # Add N-1 if available\n",
        "                                        context_text += ocr_result_n_minus_1\n",
        "                                    context_text += \"\\n--- Fin du Texte Précédent ---\\n\\n--- Image Actuelle à Transcrire ---\"\n",
        "                                    # Add context text as the first part of the prompt\n",
        "                                    prompt_parts.append(context_text)\n",
        "                                    print(f\"      Using CONTEXT system prompt (Context length: ~{len(context_text)} chars).\")\n",
        "\n",
        "\n",
        "                                # 4. Upload Image Chunk\n",
        "                                print(f\"      Uploading: {chunk_filename}...\")\n",
        "                                gemini_file = upload_to_gemini(chunk_path)\n",
        "                                if gemini_file is None:\n",
        "                                    current_ocr_result = f\"ERROR: Upload failed for {chunk_filename}\"\n",
        "                                    writer.writerow([current_ocr_result])\n",
        "                                    print(f\"      {current_ocr_result}\")\n",
        "                                    continue # Skip to next chunk\n",
        "\n",
        "                                # Add the image file as the next part of the prompt\n",
        "                                prompt_parts.append(gemini_file)\n",
        "\n",
        "                                # 5. Initialize Model (Can potentially be outside loop if system prompt doesn't change often, but safer inside for now)\n",
        "                                # Re-create model instance ensures the correct system prompt is used\n",
        "                                model = genai.GenerativeModel(\n",
        "                                    model_name=MODEL_NAME,\n",
        "                                    generation_config=GENERATION_CONFIG,\n",
        "                                    system_instruction=current_system_instruction,\n",
        "                                )\n",
        "\n",
        "                                # 6. Call Gemini API\n",
        "                                print(\"      Calling Gemini API...\")\n",
        "                                response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
        "\n",
        "                                # 7. Extract OCR Result\n",
        "                                if response and response.parts:\n",
        "                                     try:\n",
        "                                         current_ocr_result = response.text\n",
        "                                         # Optional: Remove potential leading/trailing whitespace\n",
        "                                         current_ocr_result = current_ocr_result.strip()\n",
        "                                         print(f\"      OCR result received (~{len(current_ocr_result)} chars).\")\n",
        "                                     except ValueError:\n",
        "                                         print(f\"      ⚠️ Warning: Could not extract text from response for {chunk_filename}. Response: {response.prompt_feedback}\")\n",
        "                                         current_ocr_result = f\"ERROR: No text found or content blocked ({response.prompt_feedback})\"\n",
        "                                     except Exception as e_resp:\n",
        "                                        print(f\"      🛑 Error extracting text from response: {e_resp}\")\n",
        "                                        current_ocr_result = f\"ERROR: Response parsing failed - {e_resp}\"\n",
        "                                else:\n",
        "                                    print(f\"      ⚠️ Warning: Empty or invalid response received for {chunk_filename}.\")\n",
        "                                    current_ocr_result = \"ERROR: Empty or invalid response from API\"\n",
        "\n",
        "                                # Write result to CSV\n",
        "                                writer.writerow([current_ocr_result])\n",
        "                                # print(f\"      Wrote OCR result to CSV.\")\n",
        "\n",
        "                                # 8. Update History for Next Iteration\n",
        "                                ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                                ocr_result_n_minus_1 = current_ocr_result # Store the raw result from this chunk\n",
        "\n",
        "                            except Exception as e_api:\n",
        "                                print(f\"    🛑 An error occurred during API call or processing for {chunk_filename}:\")\n",
        "                                traceback.print_exc() # Print full traceback for debugging\n",
        "                                error_message = f\"ERROR: API call failed for {chunk_filename} - {type(e_api).__name__}\"\n",
        "                                writer.writerow([error_message])\n",
        "                                # Update history with error marker to avoid propagating bad context\n",
        "                                ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                                ocr_result_n_minus_1 = error_message # Store error marker\n",
        "\n",
        "                            finally:\n",
        "                                # 9. Delete Uploaded File from Gemini\n",
        "                                if gemini_file:\n",
        "                                    delete_gemini_file(gemini_file)\n",
        "                                time.sleep(1.5) # Slightly increased delay\n",
        "\n",
        "\n",
        "            except IOError as e:\n",
        "                print(f\"🛑 Error opening or writing to CSV file {OUTPUT_CSV_FILE}: {e}\")\n",
        "            except Exception as e_main:\n",
        "                 print(f\"🛑 An unexpected error occurred during the main process:\")\n",
        "                 traceback.print_exc()\n",
        "\n",
        "            print(\"\\n--- OCR Process Complete for Folder ---\")\n",
        "            print(f\"Results saved in: {OUTPUT_CSV_FILE}\") # Display the final generated path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "86680641ed9b4b4884e323847b9598df",
            "c0b9f09ead864643b4a577e04038e935",
            "5a1f269a69174a1787ce6c2c7f128c8d",
            "086e6673b4524ce49fcda6175986536f",
            "f705514cde3e485186a48cb54714619b",
            "7def905f5b75438f9429ad2b3bbcc043",
            "f0de132605024195bcb07a546f4e607f",
            "5d895ce991224244badbab1b7316bb56",
            "42311551eda74508a56ea9e329526c90",
            "c0e4d61b22c5406c95adc21248e3eeeb",
            "ce9e8d572423473a90cd60cbf201b36f"
          ]
        },
        "id": "gbL5wmeKLrq0",
        "outputId": "d3f883a3-4e4e-4e74-86af-1612405e645b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API Key configured.\n",
            "Starting OCR process for single folder: 'Peut-on vivre en paix avec son inconscient ? (AGREG interne 2021 -note : 12)'\n",
            "  Subject detected: 'Peut-on vivre en paix avec son inconscient ?'\n",
            "  Output CSV will be saved as: '/content/Peut-on vivre en paix avec son inconscient _.csv'\n",
            "  CSV file opened successfully.\n",
            "  Wrote folder title to CSV: Peut-on vivre en paix avec son inconscient ? (AGREG interne 2021 -note : 12)\n",
            "  Found 45 chunk(s) to process.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Chunks:   0%|          | 0/45 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86680641ed9b4b4884e323847b9598df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Processing chunk 1/45: 0001.png\n",
            "      Using FIRST chunk system prompt.\n",
            "      Uploading: 0001.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~668 chars).\n",
            "\n",
            "    Processing chunk 2/45: 0002.png\n",
            "      Using CONTEXT system prompt (Context length: ~761 chars).\n",
            "      Uploading: 0002.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~641 chars).\n",
            "\n",
            "    Processing chunk 3/45: 0003.png\n",
            "      Using CONTEXT system prompt (Context length: ~1403 chars).\n",
            "      Uploading: 0003.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~328 chars).\n",
            "\n",
            "    Processing chunk 4/45: 0004.png\n",
            "      Using CONTEXT system prompt (Context length: ~1063 chars).\n",
            "      Uploading: 0004.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~645 chars).\n",
            "\n",
            "    Processing chunk 5/45: 0005.png\n",
            "      Using CONTEXT system prompt (Context length: ~1067 chars).\n",
            "      Uploading: 0005.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~631 chars).\n",
            "\n",
            "    Processing chunk 6/45: 0006.png\n",
            "      Using CONTEXT system prompt (Context length: ~1370 chars).\n",
            "      Uploading: 0006.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~544 chars).\n",
            "\n",
            "    Processing chunk 7/45: 0007.png\n",
            "      Using CONTEXT system prompt (Context length: ~1269 chars).\n",
            "      Uploading: 0007.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~561 chars).\n",
            "\n",
            "    Processing chunk 8/45: 0008.png\n",
            "      Using CONTEXT system prompt (Context length: ~1199 chars).\n",
            "      Uploading: 0008.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~496 chars).\n",
            "\n",
            "    Processing chunk 9/45: 0009.png\n",
            "      Using CONTEXT system prompt (Context length: ~1151 chars).\n",
            "      Uploading: 0009.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~668 chars).\n",
            "\n",
            "    Processing chunk 10/45: 0010.png\n",
            "      Using CONTEXT system prompt (Context length: ~1258 chars).\n",
            "      Uploading: 0010.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~265 chars).\n",
            "\n",
            "    Processing chunk 11/45: 0011.png\n",
            "      Using CONTEXT system prompt (Context length: ~1027 chars).\n",
            "      Uploading: 0011.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~563 chars).\n",
            "\n",
            "    Processing chunk 12/45: 0012.png\n",
            "      Using CONTEXT system prompt (Context length: ~922 chars).\n",
            "      Uploading: 0012.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~663 chars).\n",
            "\n",
            "    Processing chunk 13/45: 0013.png\n",
            "      Using CONTEXT system prompt (Context length: ~1320 chars).\n",
            "      Uploading: 0013.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~641 chars).\n",
            "\n",
            "    Processing chunk 14/45: 0014.png\n",
            "      Using CONTEXT system prompt (Context length: ~1398 chars).\n",
            "      Uploading: 0014.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~261 chars).\n",
            "\n",
            "    Processing chunk 15/45: 0015.png\n",
            "      Using CONTEXT system prompt (Context length: ~996 chars).\n",
            "      Uploading: 0015.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~533 chars).\n",
            "\n",
            "    Processing chunk 16/45: 0016.png\n",
            "      Using CONTEXT system prompt (Context length: ~888 chars).\n",
            "      Uploading: 0016.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~522 chars).\n",
            "\n",
            "    Processing chunk 17/45: 0017.png\n",
            "      Using CONTEXT system prompt (Context length: ~1149 chars).\n",
            "      Uploading: 0017.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~167 chars).\n",
            "\n",
            "    Processing chunk 18/45: 0018.png\n",
            "      Using CONTEXT system prompt (Context length: ~783 chars).\n",
            "      Uploading: 0018.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~627 chars).\n",
            "\n",
            "    Processing chunk 19/45: 0019.png\n",
            "      Using CONTEXT system prompt (Context length: ~888 chars).\n",
            "      Uploading: 0019.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~681 chars).\n",
            "\n",
            "    Processing chunk 20/45: 0020.png\n",
            "      Using CONTEXT system prompt (Context length: ~1402 chars).\n",
            "      Uploading: 0020.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~544 chars).\n",
            "\n",
            "    Processing chunk 21/45: 0021.png\n",
            "      Using CONTEXT system prompt (Context length: ~1319 chars).\n",
            "      Uploading: 0021.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~600 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/wrj8fbnqe3nu: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 22/45: 0022.png\n",
            "      Using CONTEXT system prompt (Context length: ~1238 chars).\n",
            "      Uploading: 0022.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~666 chars).\n",
            "\n",
            "    Processing chunk 23/45: 0023.png\n",
            "      Using CONTEXT system prompt (Context length: ~1360 chars).\n",
            "      Uploading: 0023.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~653 chars).\n",
            "\n",
            "    Processing chunk 24/45: 0024.png\n",
            "      Using CONTEXT system prompt (Context length: ~1413 chars).\n",
            "      Uploading: 0024.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~245 chars).\n",
            "\n",
            "    Processing chunk 25/45: 0025.png\n",
            "      Using CONTEXT system prompt (Context length: ~992 chars).\n",
            "      Uploading: 0025.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~575 chars).\n",
            "\n",
            "    Processing chunk 26/45: 0026.png\n",
            "      Using CONTEXT system prompt (Context length: ~914 chars).\n",
            "      Uploading: 0026.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~606 chars).\n",
            "\n",
            "    Processing chunk 27/45: 0027.png\n",
            "      Using CONTEXT system prompt (Context length: ~1275 chars).\n",
            "      Uploading: 0027.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~622 chars).\n",
            "\n",
            "    Processing chunk 28/45: 0028.png\n",
            "      Using CONTEXT system prompt (Context length: ~1322 chars).\n",
            "      Uploading: 0028.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~252 chars).\n",
            "\n",
            "    Processing chunk 29/45: 0029.png\n",
            "      Using CONTEXT system prompt (Context length: ~968 chars).\n",
            "      Uploading: 0029.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~615 chars).\n",
            "\n",
            "    Processing chunk 30/45: 0030.png\n",
            "      Using CONTEXT system prompt (Context length: ~961 chars).\n",
            "      Uploading: 0030.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~553 chars).\n",
            "\n",
            "    Processing chunk 31/45: 0031.png\n",
            "      Using CONTEXT system prompt (Context length: ~1262 chars).\n",
            "      Uploading: 0031.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~291 chars).\n",
            "\n",
            "    Processing chunk 32/45: 0032.png\n",
            "      Using CONTEXT system prompt (Context length: ~938 chars).\n",
            "      Uploading: 0032.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~501 chars).\n",
            "\n",
            "    Processing chunk 33/45: 0033.png\n",
            "      Using CONTEXT system prompt (Context length: ~886 chars).\n",
            "      Uploading: 0033.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~484 chars).\n",
            "\n",
            "    Processing chunk 34/45: 0034.png\n",
            "      Using CONTEXT system prompt (Context length: ~1079 chars).\n",
            "      Uploading: 0034.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~532 chars).\n",
            "\n",
            "    Processing chunk 35/45: 0035.png\n",
            "      Using CONTEXT system prompt (Context length: ~1110 chars).\n",
            "      Uploading: 0035.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~603 chars).\n",
            "\n",
            "    Processing chunk 36/45: 0036.png\n",
            "      Using CONTEXT system prompt (Context length: ~1229 chars).\n",
            "      Uploading: 0036.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~654 chars).\n",
            "\n",
            "    Processing chunk 37/45: 0037.png\n",
            "      Using CONTEXT system prompt (Context length: ~1351 chars).\n",
            "      Uploading: 0037.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~665 chars).\n",
            "\n",
            "    Processing chunk 38/45: 0038.png\n",
            "      Using CONTEXT system prompt (Context length: ~1413 chars).\n",
            "      Uploading: 0038.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~257 chars).\n",
            "\n",
            "    Processing chunk 39/45: 0039.png\n",
            "      Using CONTEXT system prompt (Context length: ~1016 chars).\n",
            "      Uploading: 0039.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~581 chars).\n",
            "\n",
            "    Processing chunk 40/45: 0040.png\n",
            "      Using CONTEXT system prompt (Context length: ~932 chars).\n",
            "      Uploading: 0040.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~652 chars).\n",
            "\n",
            "    Processing chunk 41/45: 0041.png\n",
            "      Using CONTEXT system prompt (Context length: ~1327 chars).\n",
            "      Uploading: 0041.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~456 chars).\n",
            "\n",
            "    Processing chunk 42/45: 0042.png\n",
            "      Using CONTEXT system prompt (Context length: ~1202 chars).\n",
            "      Uploading: 0042.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~276 chars).\n",
            "\n",
            "    Processing chunk 43/45: 0043.png\n",
            "      Using CONTEXT system prompt (Context length: ~826 chars).\n",
            "      Uploading: 0043.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~644 chars).\n",
            "\n",
            "    Processing chunk 44/45: 0044.png\n",
            "      Using CONTEXT system prompt (Context length: ~1014 chars).\n",
            "      Uploading: 0044.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~684 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/6st3aorbhpza: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 45/45: 0045.png\n",
            "      Using CONTEXT system prompt (Context length: ~1422 chars).\n",
            "      Uploading: 0045.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~153 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/rf28qylp8dba: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "--- OCR Process Complete for Folder ---\n",
            "Results saved in: /content/Peut-on vivre en paix avec son inconscient _.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src = '/content/drive/MyDrive/folder_partitioned_merged/Le bonheur est-il affaire de hasard ou de nécessité ? (AGREG interne 2022 - note : 8)'\n",
        "dst = '/content/Le bonheur est-il affaire de hasard ou de nécessité ? (AGREG interne 2022 - note : 8)'\n",
        "\n",
        "if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "shutil.copytree(src, dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3l9AvkbGgQU2",
        "outputId": "c6f63c9e-8186-4144-a14e-c58a42844901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Le bonheur est-il affaire de hasard ou de nécessité ? (AGREG interne 2022 - note : 8)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OCR avec enrengistrement en CSV au même nom\n",
        "\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import csv\n",
        "import time\n",
        "import re # For natural sorting and filename sanitization\n",
        "from tqdm.notebook import tqdm # Progress bars\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# --- Configuration ---\n",
        "# *** SET THE TARGET FOLDER HERE ***\n",
        "TARGET_FOLDER_PATH = \"/content/Le bonheur est-il affaire de hasard ou de nécessité ? (AGREG interne 2022 - note : 8)\" # Example: Set to the specific subfolder path you want to process\n",
        "\n",
        "# Output CSV file path will be generated dynamically later\n",
        "\n",
        "# Gemini API Key (ensure it's set as an environment variable)\n",
        "# ... (API Key configuration remains the same) ...\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "    print(\"Gemini API Key configured.\")\n",
        "except KeyError:\n",
        "    print(\"🛑 Error: GEMINI_API_KEY environment variable not set.\")\n",
        "    # sys.exit(1) # Optional: exit if running unattended\n",
        "except Exception as e:\n",
        "    print(f\"🛑 Error configuring Gemini API: {e}\")\n",
        "    # sys.exit(1) # Optional: exit if running unattended\n",
        "\n",
        "\n",
        "# Gemini Model Configuration)\n",
        "GENERATION_CONFIG = {\n",
        "  \"temperature\": 0.2, # Slightly lower temp might encourage more fidelity\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "MODEL_NAME = \"gemini-2.5-pro-preview-03-25\"\n",
        "\n",
        "\n",
        "# System Prompts (remain the same)\n",
        "SYSTEM_INSTRUCTION_WITH_CONTEXT = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\".\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\n",
        "Aide Contextuelle (Texte Précédent) : Pour t'aider à déchiffrer l'écriture, le texte qui précède *immédiatement* celui de l'image actuelle est fourni ci-dessous (séparé par '--- Texte Précédent ---'). Ce texte a été obtenu par OCR des sections précédentes de la dissertation. Le texte de l'image actuelle commence là où le texte précédent s'arrête (il peut y avoir un léger chevauchement de lignes dû au découpage des images).\n",
        "\n",
        "**Priorité Absolue :** Ta tâche principale reste de transcrire *exactement* ce que tu vois sur l'image actuelle. Utilise le texte précédent **uniquement** comme une aide *secondaire* pour résoudre des ambiguïtés ou déchiffrer des mots très difficiles. **Ne laisse PAS le texte précédent remplacer ou modifier ce qui est clairement visible sur l'image actuelle.** En cas de conflit entre le texte précédent et l'image, **la transcription fidèle de l'image PRÉVAUT.**\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_INSTRUCTION_FIRST_CHUNK = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\". Ceci est le tout début de la dissertation.\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\"\"\"\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def extract_folder_info(folder_path):\n",
        "    \"\"\"Extracts the subject and full name from a folder path name.\"\"\"\n",
        "    if not os.path.isdir(folder_path): return None, None\n",
        "    full_name = os.path.basename(folder_path)\n",
        "    subject = None\n",
        "    index_parenthese = full_name.find('(')\n",
        "    if index_parenthese != -1: subject = full_name[:index_parenthese].strip()\n",
        "    else: subject = full_name.strip() # Fallback\n",
        "    return subject, full_name\n",
        "\n",
        "# --- NEW: Function to sanitize filename ---\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Removes or replaces characters invalid for filenames.\"\"\"\n",
        "    # Remove leading/trailing whitespace\n",
        "    sanitized = filename.strip()\n",
        "    # Replace common invalid characters with underscore (adjust as needed)\n",
        "    sanitized = re.sub(r'[\\\\/*?:\"<>|]', '_', sanitized)\n",
        "    # Replace forward slash specifically (common in paths)\n",
        "    sanitized = sanitized.replace('/', '_')\n",
        "    # Optional: Limit length if necessary (e.g., 200 chars)\n",
        "    # max_len = 200\n",
        "    # if len(sanitized) > max_len:\n",
        "    #     name, ext = os.path.splitext(sanitized)\n",
        "    #     sanitized = name[:max_len - len(ext) - 1] + '~' + ext\n",
        "    return sanitized\n",
        "\n",
        "\n",
        "# ... (upload_to_gemini, delete_gemini_file, natural_sort_key remain the same) ...\n",
        "def upload_to_gemini(path, mime_type=\"image/png\"):\n",
        "    \"\"\"Uploads a file to Gemini and returns the File object.\"\"\"\n",
        "    try:\n",
        "        file = genai.upload_file(path, mime_type=mime_type)\n",
        "        return file\n",
        "    except Exception as e:\n",
        "        print(f\"  🛑 Error uploading file {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def delete_gemini_file(file_object):\n",
        "    \"\"\"Deletes a file from Gemini service.\"\"\"\n",
        "    if file_object:\n",
        "        try: genai.delete_file(file_object.name)\n",
        "        except Exception as e: print(f\"  ⚠️ Warning: Failed to delete file {file_object.name}: {e}\")\n",
        "\n",
        "def natural_sort_key(s):\n",
        "    \"\"\"Key for sorting strings containing numbers naturally.\"\"\"\n",
        "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
        "\n",
        "\n",
        "# --- Main Processing Logic for Single Folder ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.isdir(TARGET_FOLDER_PATH):\n",
        "        print(f\"🛑 Error: Target folder not found at '{TARGET_FOLDER_PATH}'\")\n",
        "    else:\n",
        "        print(f\"Starting OCR process for single folder: '{os.path.basename(TARGET_FOLDER_PATH)}'\")\n",
        "\n",
        "        # 1. Extract Subject and Full Name for the target folder\n",
        "        sujet_copie, nom_complet_sous_dossier = extract_folder_info(TARGET_FOLDER_PATH)\n",
        "\n",
        "        if sujet_copie is None:\n",
        "            print(f\"🛑 Error: Could not extract info from folder path '{TARGET_FOLDER_PATH}'. Cannot proceed.\")\n",
        "        else:\n",
        "            print(f\"  Subject detected: '{sujet_copie}'\")\n",
        "\n",
        "            # --- GENERATE DYNAMIC OUTPUT CSV FILENAME ---\n",
        "            parent_dir = os.path.dirname(TARGET_FOLDER_PATH) # Get directory containing the target folder\n",
        "            sanitized_subject = sanitize_filename(sujet_copie) # Sanitize the subject for use in filename\n",
        "            OUTPUT_CSV_FILE = os.path.join(parent_dir, f\"{sanitized_subject}.csv\")\n",
        "            print(f\"  Output CSV will be saved as: '{OUTPUT_CSV_FILE}'\")\n",
        "            # --- End Filename Generation ---\n",
        "\n",
        "            # --- Initialize CSV Writer ---\n",
        "            try:\n",
        "                with open(OUTPUT_CSV_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                    writer = csv.writer(csvfile)\n",
        "                    print(\"  CSV file opened successfully.\")\n",
        "\n",
        "                    # Write the folder name (title) to the CSV\n",
        "                    writer.writerow([nom_complet_sous_dossier])\n",
        "                    print(f\"  Wrote folder title to CSV: {nom_complet_sous_dossier}\")\n",
        "\n",
        "                    # 2. Find and Sort Image Chunks\n",
        "                    # ... (Chunk finding logic remains the same) ...\n",
        "                    try:\n",
        "                        all_files = [f for f in os.listdir(TARGET_FOLDER_PATH) if f.lower().endswith(\".png\")]\n",
        "                        sorted_chunks = sorted(all_files, key=natural_sort_key)\n",
        "                    except OSError as e:\n",
        "                        print(f\"  🛑 Error listing files in {TARGET_FOLDER_PATH}: {e}\")\n",
        "                        sorted_chunks = []\n",
        "\n",
        "\n",
        "                    if not sorted_chunks:\n",
        "                        print(\"  No PNG chunk files found in this folder.\")\n",
        "                    else:\n",
        "                        print(f\"  Found {len(sorted_chunks)} chunk(s) to process.\")\n",
        "\n",
        "                        # --- Variables to store previous OCR results ---\n",
        "                        ocr_result_n_minus_1 = \"\"\n",
        "                        ocr_result_n_minus_2 = \"\"\n",
        "\n",
        "                        # --- Loop through Image Chunks ---\n",
        "                        # ... (The rest of the loop logic remains exactly the same as the previous version) ...\n",
        "                        for i, chunk_filename in enumerate(tqdm(sorted_chunks, desc=\"Processing Chunks\")):\n",
        "                            chunk_path = os.path.join(TARGET_FOLDER_PATH, chunk_filename)\n",
        "                            gemini_file = None\n",
        "                            current_ocr_result = \"\" # Initialize for this chunk\n",
        "\n",
        "                            print(f\"\\n    Processing chunk {i+1}/{len(sorted_chunks)}: {chunk_filename}\")\n",
        "\n",
        "                            try:\n",
        "                                # 3. Select System Prompt and Prepare Context\n",
        "                                prompt_parts = []\n",
        "                                if i == 0:\n",
        "                                    # First chunk: No preceding context\n",
        "                                    current_system_instruction = SYSTEM_INSTRUCTION_FIRST_CHUNK.format(sujet_copie=sujet_copie)\n",
        "                                    print(\"      Using FIRST chunk system prompt.\")\n",
        "                                else:\n",
        "                                    # Subsequent chunks: Prepare context text\n",
        "                                    current_system_instruction = SYSTEM_INSTRUCTION_WITH_CONTEXT.format(sujet_copie=sujet_copie)\n",
        "                                    context_text = \"\\n--- Texte Précédent ---\\n\"\n",
        "                                    if ocr_result_n_minus_2: # Add N-2 if available\n",
        "                                        context_text += ocr_result_n_minus_2 + \"\\n\" # Add newline separator\n",
        "                                    if ocr_result_n_minus_1: # Add N-1 if available\n",
        "                                        context_text += ocr_result_n_minus_1\n",
        "                                    context_text += \"\\n--- Fin du Texte Précédent ---\\n\\n--- Image Actuelle à Transcrire ---\"\n",
        "                                    # Add context text as the first part of the prompt\n",
        "                                    prompt_parts.append(context_text)\n",
        "                                    print(f\"      Using CONTEXT system prompt (Context length: ~{len(context_text)} chars).\")\n",
        "\n",
        "\n",
        "                                # 4. Upload Image Chunk\n",
        "                                print(f\"      Uploading: {chunk_filename}...\")\n",
        "                                gemini_file = upload_to_gemini(chunk_path)\n",
        "                                if gemini_file is None:\n",
        "                                    current_ocr_result = f\"ERROR: Upload failed for {chunk_filename}\"\n",
        "                                    writer.writerow([current_ocr_result])\n",
        "                                    print(f\"      {current_ocr_result}\")\n",
        "                                    continue # Skip to next chunk\n",
        "\n",
        "                                # Add the image file as the next part of the prompt\n",
        "                                prompt_parts.append(gemini_file)\n",
        "\n",
        "                                # 5. Initialize Model (Can potentially be outside loop if system prompt doesn't change often, but safer inside for now)\n",
        "                                # Re-create model instance ensures the correct system prompt is used\n",
        "                                model = genai.GenerativeModel(\n",
        "                                    model_name=MODEL_NAME,\n",
        "                                    generation_config=GENERATION_CONFIG,\n",
        "                                    system_instruction=current_system_instruction,\n",
        "                                )\n",
        "\n",
        "                                # 6. Call Gemini API\n",
        "                                print(\"      Calling Gemini API...\")\n",
        "                                response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
        "\n",
        "                                # 7. Extract OCR Result\n",
        "                                if response and response.parts:\n",
        "                                     try:\n",
        "                                         current_ocr_result = response.text\n",
        "                                         # Optional: Remove potential leading/trailing whitespace\n",
        "                                         current_ocr_result = current_ocr_result.strip()\n",
        "                                         print(f\"      OCR result received (~{len(current_ocr_result)} chars).\")\n",
        "                                     except ValueError:\n",
        "                                         print(f\"      ⚠️ Warning: Could not extract text from response for {chunk_filename}. Response: {response.prompt_feedback}\")\n",
        "                                         current_ocr_result = f\"ERROR: No text found or content blocked ({response.prompt_feedback})\"\n",
        "                                     except Exception as e_resp:\n",
        "                                        print(f\"      🛑 Error extracting text from response: {e_resp}\")\n",
        "                                        current_ocr_result = f\"ERROR: Response parsing failed - {e_resp}\"\n",
        "                                else:\n",
        "                                    print(f\"      ⚠️ Warning: Empty or invalid response received for {chunk_filename}.\")\n",
        "                                    current_ocr_result = \"ERROR: Empty or invalid response from API\"\n",
        "\n",
        "                                # Write result to CSV\n",
        "                                writer.writerow([current_ocr_result])\n",
        "                                # print(f\"      Wrote OCR result to CSV.\")\n",
        "\n",
        "                                # 8. Update History for Next Iteration\n",
        "                                ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                                ocr_result_n_minus_1 = current_ocr_result # Store the raw result from this chunk\n",
        "\n",
        "                            except Exception as e_api:\n",
        "                                print(f\"    🛑 An error occurred during API call or processing for {chunk_filename}:\")\n",
        "                                traceback.print_exc() # Print full traceback for debugging\n",
        "                                error_message = f\"ERROR: API call failed for {chunk_filename} - {type(e_api).__name__}\"\n",
        "                                writer.writerow([error_message])\n",
        "                                # Update history with error marker to avoid propagating bad context\n",
        "                                ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                                ocr_result_n_minus_1 = error_message # Store error marker\n",
        "\n",
        "                            finally:\n",
        "                                # 9. Delete Uploaded File from Gemini\n",
        "                                if gemini_file:\n",
        "                                    delete_gemini_file(gemini_file)\n",
        "                                time.sleep(1.5) # Slightly increased delay\n",
        "\n",
        "\n",
        "            except IOError as e:\n",
        "                print(f\"🛑 Error opening or writing to CSV file {OUTPUT_CSV_FILE}: {e}\")\n",
        "            except Exception as e_main:\n",
        "                 print(f\"🛑 An unexpected error occurred during the main process:\")\n",
        "                 traceback.print_exc()\n",
        "\n",
        "            print(\"\\n--- OCR Process Complete for Folder ---\")\n",
        "            print(f\"Results saved in: {OUTPUT_CSV_FILE}\") # Display the final generated path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fcb0257035954fd4802695e50bc1089d",
            "1846e71737e54dcfb06cda1e58684d0d",
            "7912c9de38aa44f4ad49d8c766558955",
            "fdc466b072d44e858c1cf1ab232e6a3b",
            "8c2919ec86f34d67b08990ad3a9757cc",
            "8f6410cd968d405d9aae8f3fa1ce7d6b",
            "7d5988e0cb714526aa4e185ce932bb8c",
            "91935e78dbba43b0987af5f9b868fb1c",
            "ab8b92233b094267b1824c52c98ab75a",
            "643a1c6961014f12b12b9c4aae310112",
            "c74595b9b95a49b3853bb7a9702be7c7"
          ]
        },
        "cellView": "form",
        "id": "vy3pfzh6gZ__",
        "outputId": "718b2028-3eeb-43ee-b726-5325b3125195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API Key configured.\n",
            "Starting OCR process for single folder: 'Le bonheur est-il affaire de hasard ou de nécessité ? (AGREG interne 2022 - note : 8)'\n",
            "  Subject detected: 'Le bonheur est-il affaire de hasard ou de nécessité ?'\n",
            "  Output CSV will be saved as: '/content/Le bonheur est-il affaire de hasard ou de nécessité _.csv'\n",
            "  CSV file opened successfully.\n",
            "  Wrote folder title to CSV: Le bonheur est-il affaire de hasard ou de nécessité ? (AGREG interne 2022 - note : 8)\n",
            "  Found 56 chunk(s) to process.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Chunks:   0%|          | 0/56 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcb0257035954fd4802695e50bc1089d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Processing chunk 1/56: 0001.png\n",
            "      Using FIRST chunk system prompt.\n",
            "      Uploading: 0001.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~542 chars).\n",
            "\n",
            "    Processing chunk 2/56: 0002.png\n",
            "      Using CONTEXT system prompt (Context length: ~635 chars).\n",
            "      Uploading: 0002.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~647 chars).\n",
            "\n",
            "    Processing chunk 3/56: 0003.png\n",
            "      Using CONTEXT system prompt (Context length: ~1283 chars).\n",
            "      Uploading: 0003.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~257 chars).\n",
            "\n",
            "    Processing chunk 4/56: 0004.png\n",
            "      Using CONTEXT system prompt (Context length: ~998 chars).\n",
            "      Uploading: 0004.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~637 chars).\n",
            "\n",
            "    Processing chunk 5/56: 0005.png\n",
            "      Using CONTEXT system prompt (Context length: ~988 chars).\n",
            "      Uploading: 0005.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~604 chars).\n",
            "\n",
            "    Processing chunk 6/56: 0006.png\n",
            "      Using CONTEXT system prompt (Context length: ~1335 chars).\n",
            "      Uploading: 0006.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~487 chars).\n",
            "\n",
            "    Processing chunk 7/56: 0007.png\n",
            "      Using CONTEXT system prompt (Context length: ~1185 chars).\n",
            "      Uploading: 0007.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~560 chars).\n",
            "\n",
            "    Processing chunk 8/56: 0008.png\n",
            "      Using CONTEXT system prompt (Context length: ~1141 chars).\n",
            "      Uploading: 0008.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~324 chars).\n",
            "\n",
            "    Processing chunk 9/56: 0009.png\n",
            "      Using CONTEXT system prompt (Context length: ~978 chars).\n",
            "      Uploading: 0009.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~634 chars).\n",
            "\n",
            "    Processing chunk 10/56: 0010.png\n",
            "      Using CONTEXT system prompt (Context length: ~1052 chars).\n",
            "      Uploading: 0010.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~207 chars).\n",
            "\n",
            "    Processing chunk 11/56: 0011.png\n",
            "      Using CONTEXT system prompt (Context length: ~935 chars).\n",
            "      Uploading: 0011.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~596 chars).\n",
            "\n",
            "    Processing chunk 12/56: 0012.png\n",
            "      Using CONTEXT system prompt (Context length: ~897 chars).\n",
            "      Uploading: 0012.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~643 chars).\n",
            "\n",
            "    Processing chunk 13/56: 0013.png\n",
            "      Using CONTEXT system prompt (Context length: ~1333 chars).\n",
            "      Uploading: 0013.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~580 chars).\n",
            "\n",
            "    Processing chunk 14/56: 0014.png\n",
            "      Using CONTEXT system prompt (Context length: ~1317 chars).\n",
            "      Uploading: 0014.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~216 chars).\n",
            "\n",
            "    Processing chunk 15/56: 0015.png\n",
            "      Using CONTEXT system prompt (Context length: ~890 chars).\n",
            "      Uploading: 0015.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~626 chars).\n",
            "\n",
            "    Processing chunk 16/56: 0016.png\n",
            "      Using CONTEXT system prompt (Context length: ~936 chars).\n",
            "      Uploading: 0016.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~593 chars).\n",
            "\n",
            "    Processing chunk 17/56: 0017.png\n",
            "      Using CONTEXT system prompt (Context length: ~1313 chars).\n",
            "      Uploading: 0017.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~261 chars).\n",
            "\n",
            "    Processing chunk 18/56: 0018.png\n",
            "      Using CONTEXT system prompt (Context length: ~948 chars).\n",
            "      Uploading: 0018.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~614 chars).\n",
            "\n",
            "    Processing chunk 19/56: 0019.png\n",
            "      Using CONTEXT system prompt (Context length: ~969 chars).\n",
            "      Uploading: 0019.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~630 chars).\n",
            "\n",
            "    Processing chunk 20/56: 0020.png\n",
            "      Using CONTEXT system prompt (Context length: ~1338 chars).\n",
            "      Uploading: 0020.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~452 chars).\n",
            "\n",
            "    Processing chunk 21/56: 0021.png\n",
            "      Using CONTEXT system prompt (Context length: ~1176 chars).\n",
            "      Uploading: 0021.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~521 chars).\n",
            "\n",
            "    Processing chunk 22/56: 0022.png\n",
            "      Using CONTEXT system prompt (Context length: ~1067 chars).\n",
            "      Uploading: 0022.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~602 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/3thxwjtwix76: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 23/56: 0023.png\n",
            "      Using CONTEXT system prompt (Context length: ~1217 chars).\n",
            "      Uploading: 0023.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~453 chars).\n",
            "\n",
            "    Processing chunk 24/56: 0024.png\n",
            "      Using CONTEXT system prompt (Context length: ~1149 chars).\n",
            "      Uploading: 0024.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~535 chars).\n",
            "\n",
            "    Processing chunk 25/56: 0025.png\n",
            "      Using CONTEXT system prompt (Context length: ~1082 chars).\n",
            "      Uploading: 0025.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~567 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/90uan0pah2a7: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "    Processing chunk 26/56: 0026.png\n",
            "      Using CONTEXT system prompt (Context length: ~1196 chars).\n",
            "      Uploading: 0026.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~603 chars).\n",
            "\n",
            "    Processing chunk 27/56: 0027.png\n",
            "      Using CONTEXT system prompt (Context length: ~1264 chars).\n",
            "      Uploading: 0027.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~202 chars).\n",
            "\n",
            "    Processing chunk 28/56: 0028.png\n",
            "      Using CONTEXT system prompt (Context length: ~899 chars).\n",
            "      Uploading: 0028.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~606 chars).\n",
            "\n",
            "    Processing chunk 29/56: 0029.png\n",
            "      Using CONTEXT system prompt (Context length: ~902 chars).\n",
            "      Uploading: 0029.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~584 chars).\n",
            "\n",
            "    Processing chunk 30/56: 0030.png\n",
            "      Using CONTEXT system prompt (Context length: ~1284 chars).\n",
            "      Uploading: 0030.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~246 chars).\n",
            "\n",
            "    Processing chunk 31/56: 0031.png\n",
            "      Using CONTEXT system prompt (Context length: ~924 chars).\n",
            "      Uploading: 0031.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~615 chars).\n",
            "\n",
            "    Processing chunk 32/56: 0032.png\n",
            "      Using CONTEXT system prompt (Context length: ~955 chars).\n",
            "      Uploading: 0032.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~575 chars).\n",
            "\n",
            "    Processing chunk 33/56: 0033.png\n",
            "      Using CONTEXT system prompt (Context length: ~1284 chars).\n",
            "      Uploading: 0033.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~431 chars).\n",
            "\n",
            "    Processing chunk 34/56: 0034.png\n",
            "      Using CONTEXT system prompt (Context length: ~1100 chars).\n",
            "      Uploading: 0034.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~573 chars).\n",
            "\n",
            "    Processing chunk 35/56: 0035.png\n",
            "      Using CONTEXT system prompt (Context length: ~1098 chars).\n",
            "      Uploading: 0035.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~632 chars).\n",
            "\n",
            "    Processing chunk 36/56: 0036.png\n",
            "      Using CONTEXT system prompt (Context length: ~1299 chars).\n",
            "      Uploading: 0036.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~590 chars).\n",
            "\n",
            "    Processing chunk 37/56: 0037.png\n",
            "      Using CONTEXT system prompt (Context length: ~1316 chars).\n",
            "      Uploading: 0037.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~170 chars).\n",
            "\n",
            "    Processing chunk 38/56: 0038.png\n",
            "      Using CONTEXT system prompt (Context length: ~854 chars).\n",
            "      Uploading: 0038.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~276 chars).\n",
            "\n",
            "    Processing chunk 39/56: 0039.png\n",
            "      Using CONTEXT system prompt (Context length: ~540 chars).\n",
            "      Uploading: 0039.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~602 chars).\n",
            "\n",
            "    Processing chunk 40/56: 0040.png\n",
            "      Using CONTEXT system prompt (Context length: ~972 chars).\n",
            "      Uploading: 0040.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~558 chars).\n",
            "\n",
            "    Processing chunk 41/56: 0041.png\n",
            "      Using CONTEXT system prompt (Context length: ~1254 chars).\n",
            "      Uploading: 0041.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~176 chars).\n",
            "\n",
            "    Processing chunk 42/56: 0042.png\n",
            "      Using CONTEXT system prompt (Context length: ~828 chars).\n",
            "      Uploading: 0042.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~543 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/z29o35bq3vnt: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 43/56: 0043.png\n",
            "      Using CONTEXT system prompt (Context length: ~813 chars).\n",
            "      Uploading: 0043.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~605 chars).\n",
            "\n",
            "    Processing chunk 44/56: 0044.png\n",
            "      Using CONTEXT system prompt (Context length: ~1242 chars).\n",
            "      Uploading: 0044.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~285 chars).\n",
            "\n",
            "    Processing chunk 45/56: 0045.png\n",
            "      Using CONTEXT system prompt (Context length: ~984 chars).\n",
            "      Uploading: 0045.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~612 chars).\n",
            "\n",
            "    Processing chunk 46/56: 0046.png\n",
            "      Using CONTEXT system prompt (Context length: ~991 chars).\n",
            "      Uploading: 0046.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~603 chars).\n",
            "\n",
            "    Processing chunk 47/56: 0047.png\n",
            "      Using CONTEXT system prompt (Context length: ~1309 chars).\n",
            "      Uploading: 0047.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~434 chars).\n",
            "\n",
            "    Processing chunk 48/56: 0048.png\n",
            "      Using CONTEXT system prompt (Context length: ~1131 chars).\n",
            "      Uploading: 0048.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~525 chars).\n",
            "\n",
            "    Processing chunk 49/56: 0049.png\n",
            "      Using CONTEXT system prompt (Context length: ~1053 chars).\n",
            "      Uploading: 0049.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~555 chars).\n",
            "\n",
            "    Processing chunk 50/56: 0050.png\n",
            "      Using CONTEXT system prompt (Context length: ~1174 chars).\n",
            "      Uploading: 0050.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~588 chars).\n",
            "\n",
            "    Processing chunk 51/56: 0051.png\n",
            "      Using CONTEXT system prompt (Context length: ~1237 chars).\n",
            "      Uploading: 0051.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~191 chars).\n",
            "\n",
            "    Processing chunk 52/56: 0052.png\n",
            "      Using CONTEXT system prompt (Context length: ~873 chars).\n",
            "      Uploading: 0052.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~526 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/nxukyoqn3vr: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 53/56: 0053.png\n",
            "      Using CONTEXT system prompt (Context length: ~811 chars).\n",
            "      Uploading: 0053.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~363 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/jx513jh6pgvt: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 54/56: 0054.png\n",
            "      Using CONTEXT system prompt (Context length: ~983 chars).\n",
            "      Uploading: 0054.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~606 chars).\n",
            "\n",
            "    Processing chunk 55/56: 0055.png\n",
            "      Using CONTEXT system prompt (Context length: ~1063 chars).\n",
            "      Uploading: 0055.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~185 chars).\n",
            "\n",
            "    Processing chunk 56/56: 0056.png\n",
            "      Using CONTEXT system prompt (Context length: ~885 chars).\n",
            "      Uploading: 0056.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~439 chars).\n",
            "\n",
            "--- OCR Process Complete for Folder ---\n",
            "Results saved in: /content/Le bonheur est-il affaire de hasard ou de nécessité _.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src = '/content/drive/MyDrive/folder_partitioned_merged/Le bonheur est-il affaire de hasard ou de nécessité ? (AGREG interne 2022 - note : 8)'\n",
        "dst = '/content/Le bonheur affaire de hasard ou de nécessité (attention)'\n",
        "\n",
        "if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "shutil.copytree(src, dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E1MvlIEmg38O",
        "outputId": "7af3fc47-b3ab-43a0-915e-6b7e8a8914e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Le bonheur affaire de hasard ou de nécessité (attention)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src = '/content/drive/MyDrive/folder_partitioned_merged/Peut-on vivre en paix avec son inconscient ? (AGREG interne 2021 - note : 14) (1)'\n",
        "dst = '/content/Peut-on vivre en paix avec son inconscient ? (AGREG interne 2021 - note : 14) (1)'\n",
        "\n",
        "if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "shutil.copytree(src, dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yVI4X4dahCxP",
        "outputId": "4ecbd2e2-3545-4a37-da8b-4d8685eb7712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Peut-on vivre en paix avec son inconscient ? (AGREG interne 2021 - note : 14) (1)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OCR avec enrengistrement en CSV au même nom\n",
        "\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import csv\n",
        "import time\n",
        "import re # For natural sorting and filename sanitization\n",
        "from tqdm.notebook import tqdm # Progress bars\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# --- Configuration ---\n",
        "# *** SET THE TARGET FOLDER HERE ***\n",
        "TARGET_FOLDER_PATH = \"/content/Peut-on vivre en paix avec son inconscient  - bis? (AGREG interne 2021 - note : 14) (1)\" # Example: Set to the specific subfolder path you want to process\n",
        "\n",
        "# Output CSV file path will be generated dynamically later\n",
        "\n",
        "# Gemini API Key (ensure it's set as an environment variable)\n",
        "# ... (API Key configuration remains the same) ...\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "    print(\"Gemini API Key configured.\")\n",
        "except KeyError:\n",
        "    print(\"🛑 Error: GEMINI_API_KEY environment variable not set.\")\n",
        "    # sys.exit(1) # Optional: exit if running unattended\n",
        "except Exception as e:\n",
        "    print(f\"🛑 Error configuring Gemini API: {e}\")\n",
        "    # sys.exit(1) # Optional: exit if running unattended\n",
        "\n",
        "\n",
        "# Gemini Model Configuration)\n",
        "GENERATION_CONFIG = {\n",
        "  \"temperature\": 0.2, # Slightly lower temp might encourage more fidelity\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "MODEL_NAME = \"gemini-2.5-pro-preview-03-25\"\n",
        "\n",
        "\n",
        "# System Prompts (remain the same)\n",
        "SYSTEM_INSTRUCTION_WITH_CONTEXT = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\".\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\n",
        "Aide Contextuelle (Texte Précédent) : Pour t'aider à déchiffrer l'écriture, le texte qui précède *immédiatement* celui de l'image actuelle est fourni ci-dessous (séparé par '--- Texte Précédent ---'). Ce texte a été obtenu par OCR des sections précédentes de la dissertation. Le texte de l'image actuelle commence là où le texte précédent s'arrête (il peut y avoir un léger chevauchement de lignes dû au découpage des images).\n",
        "\n",
        "**Priorité Absolue :** Ta tâche principale reste de transcrire *exactement* ce que tu vois sur l'image actuelle. Utilise le texte précédent **uniquement** comme une aide *secondaire* pour résoudre des ambiguïtés ou déchiffrer des mots très difficiles. **Ne laisse PAS le texte précédent remplacer ou modifier ce qui est clairement visible sur l'image actuelle.** En cas de conflit entre le texte précédent et l'image, **la transcription fidèle de l'image PRÉVAUT.**\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_INSTRUCTION_FIRST_CHUNK = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\". Ceci est le tout début de la dissertation.\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\"\"\"\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def extract_folder_info(folder_path):\n",
        "    \"\"\"Extracts the subject and full name from a folder path name.\"\"\"\n",
        "    if not os.path.isdir(folder_path): return None, None\n",
        "    full_name = os.path.basename(folder_path)\n",
        "    subject = None\n",
        "    index_parenthese = full_name.find('(')\n",
        "    if index_parenthese != -1: subject = full_name[:index_parenthese].strip()\n",
        "    else: subject = full_name.strip() # Fallback\n",
        "    return subject, full_name\n",
        "\n",
        "# --- NEW: Function to sanitize filename ---\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Removes or replaces characters invalid for filenames.\"\"\"\n",
        "    # Remove leading/trailing whitespace\n",
        "    sanitized = filename.strip()\n",
        "    # Replace common invalid characters with underscore (adjust as needed)\n",
        "    sanitized = re.sub(r'[\\\\/*?:\"<>|]', '_', sanitized)\n",
        "    # Replace forward slash specifically (common in paths)\n",
        "    sanitized = sanitized.replace('/', '_')\n",
        "    # Optional: Limit length if necessary (e.g., 200 chars)\n",
        "    # max_len = 200\n",
        "    # if len(sanitized) > max_len:\n",
        "    #     name, ext = os.path.splitext(sanitized)\n",
        "    #     sanitized = name[:max_len - len(ext) - 1] + '~' + ext\n",
        "    return sanitized\n",
        "\n",
        "\n",
        "# ... (upload_to_gemini, delete_gemini_file, natural_sort_key remain the same) ...\n",
        "def upload_to_gemini(path, mime_type=\"image/png\"):\n",
        "    \"\"\"Uploads a file to Gemini and returns the File object.\"\"\"\n",
        "    try:\n",
        "        file = genai.upload_file(path, mime_type=mime_type)\n",
        "        return file\n",
        "    except Exception as e:\n",
        "        print(f\"  🛑 Error uploading file {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def delete_gemini_file(file_object):\n",
        "    \"\"\"Deletes a file from Gemini service.\"\"\"\n",
        "    if file_object:\n",
        "        try: genai.delete_file(file_object.name)\n",
        "        except Exception as e: print(f\"  ⚠️ Warning: Failed to delete file {file_object.name}: {e}\")\n",
        "\n",
        "def natural_sort_key(s):\n",
        "    \"\"\"Key for sorting strings containing numbers naturally.\"\"\"\n",
        "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
        "\n",
        "\n",
        "# --- Main Processing Logic for Single Folder ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.isdir(TARGET_FOLDER_PATH):\n",
        "        print(f\"🛑 Error: Target folder not found at '{TARGET_FOLDER_PATH}'\")\n",
        "    else:\n",
        "        print(f\"Starting OCR process for single folder: '{os.path.basename(TARGET_FOLDER_PATH)}'\")\n",
        "\n",
        "        # 1. Extract Subject and Full Name for the target folder\n",
        "        sujet_copie, nom_complet_sous_dossier = extract_folder_info(TARGET_FOLDER_PATH)\n",
        "\n",
        "        if sujet_copie is None:\n",
        "            print(f\"🛑 Error: Could not extract info from folder path '{TARGET_FOLDER_PATH}'. Cannot proceed.\")\n",
        "        else:\n",
        "            print(f\"  Subject detected: '{sujet_copie}'\")\n",
        "\n",
        "            # --- GENERATE DYNAMIC OUTPUT CSV FILENAME ---\n",
        "            parent_dir = os.path.dirname(TARGET_FOLDER_PATH) # Get directory containing the target folder\n",
        "            sanitized_subject = sanitize_filename(sujet_copie) # Sanitize the subject for use in filename\n",
        "            OUTPUT_CSV_FILE = os.path.join(parent_dir, f\"{sanitized_subject}.csv\")\n",
        "            print(f\"  Output CSV will be saved as: '{OUTPUT_CSV_FILE}'\")\n",
        "            # --- End Filename Generation ---\n",
        "\n",
        "            # --- Initialize CSV Writer ---\n",
        "            try:\n",
        "                with open(OUTPUT_CSV_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                    writer = csv.writer(csvfile)\n",
        "                    print(\"  CSV file opened successfully.\")\n",
        "\n",
        "                    # Write the folder name (title) to the CSV\n",
        "                    writer.writerow([nom_complet_sous_dossier])\n",
        "                    print(f\"  Wrote folder title to CSV: {nom_complet_sous_dossier}\")\n",
        "\n",
        "                    # 2. Find and Sort Image Chunks\n",
        "                    # ... (Chunk finding logic remains the same) ...\n",
        "                    try:\n",
        "                        all_files = [f for f in os.listdir(TARGET_FOLDER_PATH) if f.lower().endswith(\".png\")]\n",
        "                        sorted_chunks = sorted(all_files, key=natural_sort_key)\n",
        "                    except OSError as e:\n",
        "                        print(f\"  🛑 Error listing files in {TARGET_FOLDER_PATH}: {e}\")\n",
        "                        sorted_chunks = []\n",
        "\n",
        "\n",
        "                    if not sorted_chunks:\n",
        "                        print(\"  No PNG chunk files found in this folder.\")\n",
        "                    else:\n",
        "                        print(f\"  Found {len(sorted_chunks)} chunk(s) to process.\")\n",
        "\n",
        "                        # --- Variables to store previous OCR results ---\n",
        "                        ocr_result_n_minus_1 = \"\"\n",
        "                        ocr_result_n_minus_2 = \"\"\n",
        "\n",
        "                        # --- Loop through Image Chunks ---\n",
        "                        # ... (The rest of the loop logic remains exactly the same as the previous version) ...\n",
        "                        for i, chunk_filename in enumerate(tqdm(sorted_chunks, desc=\"Processing Chunks\")):\n",
        "                            chunk_path = os.path.join(TARGET_FOLDER_PATH, chunk_filename)\n",
        "                            gemini_file = None\n",
        "                            current_ocr_result = \"\" # Initialize for this chunk\n",
        "\n",
        "                            print(f\"\\n    Processing chunk {i+1}/{len(sorted_chunks)}: {chunk_filename}\")\n",
        "\n",
        "                            try:\n",
        "                                # 3. Select System Prompt and Prepare Context\n",
        "                                prompt_parts = []\n",
        "                                if i == 0:\n",
        "                                    # First chunk: No preceding context\n",
        "                                    current_system_instruction = SYSTEM_INSTRUCTION_FIRST_CHUNK.format(sujet_copie=sujet_copie)\n",
        "                                    print(\"      Using FIRST chunk system prompt.\")\n",
        "                                else:\n",
        "                                    # Subsequent chunks: Prepare context text\n",
        "                                    current_system_instruction = SYSTEM_INSTRUCTION_WITH_CONTEXT.format(sujet_copie=sujet_copie)\n",
        "                                    context_text = \"\\n--- Texte Précédent ---\\n\"\n",
        "                                    if ocr_result_n_minus_2: # Add N-2 if available\n",
        "                                        context_text += ocr_result_n_minus_2 + \"\\n\" # Add newline separator\n",
        "                                    if ocr_result_n_minus_1: # Add N-1 if available\n",
        "                                        context_text += ocr_result_n_minus_1\n",
        "                                    context_text += \"\\n--- Fin du Texte Précédent ---\\n\\n--- Image Actuelle à Transcrire ---\"\n",
        "                                    # Add context text as the first part of the prompt\n",
        "                                    prompt_parts.append(context_text)\n",
        "                                    print(f\"      Using CONTEXT system prompt (Context length: ~{len(context_text)} chars).\")\n",
        "\n",
        "\n",
        "                                # 4. Upload Image Chunk\n",
        "                                print(f\"      Uploading: {chunk_filename}...\")\n",
        "                                gemini_file = upload_to_gemini(chunk_path)\n",
        "                                if gemini_file is None:\n",
        "                                    current_ocr_result = f\"ERROR: Upload failed for {chunk_filename}\"\n",
        "                                    writer.writerow([current_ocr_result])\n",
        "                                    print(f\"      {current_ocr_result}\")\n",
        "                                    continue # Skip to next chunk\n",
        "\n",
        "                                # Add the image file as the next part of the prompt\n",
        "                                prompt_parts.append(gemini_file)\n",
        "\n",
        "                                # 5. Initialize Model (Can potentially be outside loop if system prompt doesn't change often, but safer inside for now)\n",
        "                                # Re-create model instance ensures the correct system prompt is used\n",
        "                                model = genai.GenerativeModel(\n",
        "                                    model_name=MODEL_NAME,\n",
        "                                    generation_config=GENERATION_CONFIG,\n",
        "                                    system_instruction=current_system_instruction,\n",
        "                                )\n",
        "\n",
        "                                # 6. Call Gemini API\n",
        "                                print(\"      Calling Gemini API...\")\n",
        "                                response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
        "\n",
        "                                # 7. Extract OCR Result\n",
        "                                if response and response.parts:\n",
        "                                     try:\n",
        "                                         current_ocr_result = response.text\n",
        "                                         # Optional: Remove potential leading/trailing whitespace\n",
        "                                         current_ocr_result = current_ocr_result.strip()\n",
        "                                         print(f\"      OCR result received (~{len(current_ocr_result)} chars).\")\n",
        "                                     except ValueError:\n",
        "                                         print(f\"      ⚠️ Warning: Could not extract text from response for {chunk_filename}. Response: {response.prompt_feedback}\")\n",
        "                                         current_ocr_result = f\"ERROR: No text found or content blocked ({response.prompt_feedback})\"\n",
        "                                     except Exception as e_resp:\n",
        "                                        print(f\"      🛑 Error extracting text from response: {e_resp}\")\n",
        "                                        current_ocr_result = f\"ERROR: Response parsing failed - {e_resp}\"\n",
        "                                else:\n",
        "                                    print(f\"      ⚠️ Warning: Empty or invalid response received for {chunk_filename}.\")\n",
        "                                    current_ocr_result = \"ERROR: Empty or invalid response from API\"\n",
        "\n",
        "                                # Write result to CSV\n",
        "                                writer.writerow([current_ocr_result])\n",
        "                                # print(f\"      Wrote OCR result to CSV.\")\n",
        "\n",
        "                                # 8. Update History for Next Iteration\n",
        "                                ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                                ocr_result_n_minus_1 = current_ocr_result # Store the raw result from this chunk\n",
        "\n",
        "                            except Exception as e_api:\n",
        "                                print(f\"    🛑 An error occurred during API call or processing for {chunk_filename}:\")\n",
        "                                traceback.print_exc() # Print full traceback for debugging\n",
        "                                error_message = f\"ERROR: API call failed for {chunk_filename} - {type(e_api).__name__}\"\n",
        "                                writer.writerow([error_message])\n",
        "                                # Update history with error marker to avoid propagating bad context\n",
        "                                ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                                ocr_result_n_minus_1 = error_message # Store error marker\n",
        "\n",
        "                            finally:\n",
        "                                # 9. Delete Uploaded File from Gemini\n",
        "                                if gemini_file:\n",
        "                                    delete_gemini_file(gemini_file)\n",
        "                                time.sleep(1.5) # Slightly increased delay\n",
        "\n",
        "\n",
        "            except IOError as e:\n",
        "                print(f\"🛑 Error opening or writing to CSV file {OUTPUT_CSV_FILE}: {e}\")\n",
        "            except Exception as e_main:\n",
        "                 print(f\"🛑 An unexpected error occurred during the main process:\")\n",
        "                 traceback.print_exc()\n",
        "\n",
        "            print(\"\\n--- OCR Process Complete for Folder ---\")\n",
        "            print(f\"Results saved in: {OUTPUT_CSV_FILE}\") # Display the final generated path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "093b38b98c3d4473be2021c7f6ee648b",
            "1288ee98ac4242dbb5c011bf3d1f5851",
            "87367c60d82c4b34b576a2453dcdbef3",
            "89441e59841847f1aa1df6b22e640468",
            "5249e308bda847888542129f63d601dd",
            "885087257319416c8ce7e0338a0d9b95",
            "491137e96d904aec8bd4de162aa6cb0a",
            "9154ee28657c4fb286ef4b026c5c6867",
            "79051d52de5e4e92bbbb01df89ec731c",
            "d77957e1019f4d3484374282d7e26f8d",
            "0eacf43db0e242478ce4df17cecd36e1"
          ]
        },
        "cellView": "form",
        "id": "937muy0FhOAA",
        "outputId": "f1c8c1e5-52ae-4036-b568-ee3b42b570dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API Key configured.\n",
            "Starting OCR process for single folder: 'Peut-on vivre en paix avec son inconscient  - bis? (AGREG interne 2021 - note : 14) (1)'\n",
            "  Subject detected: 'Peut-on vivre en paix avec son inconscient  - bis?'\n",
            "  Output CSV will be saved as: '/content/Peut-on vivre en paix avec son inconscient  - bis_.csv'\n",
            "  CSV file opened successfully.\n",
            "  Wrote folder title to CSV: Peut-on vivre en paix avec son inconscient  - bis? (AGREG interne 2021 - note : 14) (1)\n",
            "  Found 43 chunk(s) to process.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Chunks:   0%|          | 0/43 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "093b38b98c3d4473be2021c7f6ee648b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Processing chunk 1/43: 0001.png\n",
            "      Using FIRST chunk system prompt.\n",
            "      Uploading: 0001.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~836 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/xw7pgnmaux9z: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 2/43: 0002.png\n",
            "      Using CONTEXT system prompt (Context length: ~929 chars).\n",
            "      Uploading: 0002.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~823 chars).\n",
            "\n",
            "    Processing chunk 3/43: 0003.png\n",
            "      Using CONTEXT system prompt (Context length: ~1753 chars).\n",
            "      Uploading: 0003.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~403 chars).\n",
            "\n",
            "    Processing chunk 4/43: 0004.png\n",
            "      Using CONTEXT system prompt (Context length: ~1320 chars).\n",
            "      Uploading: 0004.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~717 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/aodefgar70kc: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 5/43: 0005.png\n",
            "      Using CONTEXT system prompt (Context length: ~1214 chars).\n",
            "      Uploading: 0005.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~781 chars).\n",
            "\n",
            "    Processing chunk 6/43: 0006.png\n",
            "      Using CONTEXT system prompt (Context length: ~1592 chars).\n",
            "      Uploading: 0006.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~638 chars).\n",
            "\n",
            "    Processing chunk 7/43: 0007.png\n",
            "      Using CONTEXT system prompt (Context length: ~1513 chars).\n",
            "      Uploading: 0007.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~5 chars).\n",
            "\n",
            "    Processing chunk 8/43: 0008.png\n",
            "      Using CONTEXT system prompt (Context length: ~737 chars).\n",
            "      Uploading: 0008.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~469 chars).\n",
            "\n",
            "    Processing chunk 9/43: 0009.png\n",
            "      Using CONTEXT system prompt (Context length: ~568 chars).\n",
            "      Uploading: 0009.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~813 chars).\n",
            "\n",
            "    Processing chunk 10/43: 0010.png\n",
            "      Using CONTEXT system prompt (Context length: ~1376 chars).\n",
            "      Uploading: 0010.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~831 chars).\n",
            "\n",
            "    Processing chunk 11/43: 0011.png\n",
            "      Using CONTEXT system prompt (Context length: ~1738 chars).\n",
            "      Uploading: 0011.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~292 chars).\n",
            "\n",
            "    Processing chunk 12/43: 0012.png\n",
            "      Using CONTEXT system prompt (Context length: ~1217 chars).\n",
            "      Uploading: 0012.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~669 chars).\n",
            "\n",
            "    Processing chunk 13/43: 0013.png\n",
            "      Using CONTEXT system prompt (Context length: ~1055 chars).\n",
            "      Uploading: 0013.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~825 chars).\n",
            "\n",
            "    Processing chunk 14/43: 0014.png\n",
            "      Using CONTEXT system prompt (Context length: ~1588 chars).\n",
            "      Uploading: 0014.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~699 chars).\n",
            "\n",
            "    Processing chunk 15/43: 0015.png\n",
            "      Using CONTEXT system prompt (Context length: ~1618 chars).\n",
            "      Uploading: 0015.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~371 chars).\n",
            "\n",
            "    Processing chunk 16/43: 0016.png\n",
            "      Using CONTEXT system prompt (Context length: ~1164 chars).\n",
            "      Uploading: 0016.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~832 chars).\n",
            "\n",
            "    Processing chunk 17/43: 0017.png\n",
            "      Using CONTEXT system prompt (Context length: ~1297 chars).\n",
            "      Uploading: 0017.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~844 chars).\n",
            "\n",
            "    Processing chunk 18/43: 0018.png\n",
            "      Using CONTEXT system prompt (Context length: ~1770 chars).\n",
            "      Uploading: 0018.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~343 chars).\n",
            "\n",
            "    Processing chunk 19/43: 0019.png\n",
            "      Using CONTEXT system prompt (Context length: ~1281 chars).\n",
            "      Uploading: 0019.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~479 chars).\n",
            "\n",
            "    Processing chunk 20/43: 0020.png\n",
            "      Using CONTEXT system prompt (Context length: ~916 chars).\n",
            "      Uploading: 0020.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~802 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/4f2shapnztvd: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 21/43: 0021.png\n",
            "      Using CONTEXT system prompt (Context length: ~1375 chars).\n",
            "      Uploading: 0021.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~753 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/8ld7nbwmcr0r: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "    Processing chunk 22/43: 0022.png\n",
            "      Using CONTEXT system prompt (Context length: ~1649 chars).\n",
            "      Uploading: 0022.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~63 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/5laz0cip9vll: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 23/43: 0023.png\n",
            "      Using CONTEXT system prompt (Context length: ~910 chars).\n",
            "      Uploading: 0023.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~723 chars).\n",
            "\n",
            "    Processing chunk 24/43: 0024.png\n",
            "      Using CONTEXT system prompt (Context length: ~880 chars).\n",
            "      Uploading: 0024.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~849 chars).\n",
            "\n",
            "    Processing chunk 25/43: 0025.png\n",
            "      Using CONTEXT system prompt (Context length: ~1666 chars).\n",
            "      Uploading: 0025.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~800 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/qxjqe4n2v2h: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 26/43: 0026.png\n",
            "      Using CONTEXT system prompt (Context length: ~1743 chars).\n",
            "      Uploading: 0026.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~332 chars).\n",
            "\n",
            "    Processing chunk 27/43: 0027.png\n",
            "      Using CONTEXT system prompt (Context length: ~1226 chars).\n",
            "      Uploading: 0027.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~660 chars).\n",
            "\n",
            "    Processing chunk 28/43: 0028.png\n",
            "      Using CONTEXT system prompt (Context length: ~1086 chars).\n",
            "      Uploading: 0028.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~770 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/l9m8tuy6r3t4: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "    Processing chunk 29/43: 0029.png\n",
            "      Using CONTEXT system prompt (Context length: ~1524 chars).\n",
            "      Uploading: 0029.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~812 chars).\n",
            "\n",
            "    Processing chunk 30/43: 0030.png\n",
            "      Using CONTEXT system prompt (Context length: ~1676 chars).\n",
            "      Uploading: 0030.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~330 chars).\n",
            "\n",
            "    Processing chunk 31/43: 0031.png\n",
            "      Using CONTEXT system prompt (Context length: ~1236 chars).\n",
            "      Uploading: 0031.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~578 chars).\n",
            "\n",
            "    Processing chunk 32/43: 0032.png\n",
            "      Using CONTEXT system prompt (Context length: ~1002 chars).\n",
            "      Uploading: 0032.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~634 chars).\n",
            "\n",
            "    Processing chunk 33/43: 0033.png\n",
            "      Using CONTEXT system prompt (Context length: ~1306 chars).\n",
            "      Uploading: 0033.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~360 chars).\n",
            "\n",
            "    Processing chunk 34/43: 0034.png\n",
            "      Using CONTEXT system prompt (Context length: ~1088 chars).\n",
            "      Uploading: 0034.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~707 chars).\n",
            "\n",
            "    Processing chunk 35/43: 0035.png\n",
            "      Using CONTEXT system prompt (Context length: ~1161 chars).\n",
            "      Uploading: 0035.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~647 chars).\n",
            "\n",
            "    Processing chunk 36/43: 0036.png\n",
            "      Using CONTEXT system prompt (Context length: ~1448 chars).\n",
            "      Uploading: 0036.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~702 chars).\n",
            "\n",
            "    Processing chunk 37/43: 0037.png\n",
            "      Using CONTEXT system prompt (Context length: ~1443 chars).\n",
            "      Uploading: 0037.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~59 chars).\n",
            "\n",
            "    Processing chunk 38/43: 0038.png\n",
            "      Using CONTEXT system prompt (Context length: ~855 chars).\n",
            "      Uploading: 0038.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~721 chars).\n",
            "\n",
            "    Processing chunk 39/43: 0039.png\n",
            "      Using CONTEXT system prompt (Context length: ~874 chars).\n",
            "      Uploading: 0039.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~796 chars).\n",
            "\n",
            "    Processing chunk 40/43: 0040.png\n",
            "      Using CONTEXT system prompt (Context length: ~1611 chars).\n",
            "      Uploading: 0040.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~713 chars).\n",
            "\n",
            "    Processing chunk 41/43: 0041.png\n",
            "      Using CONTEXT system prompt (Context length: ~1603 chars).\n",
            "      Uploading: 0041.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~111 chars).\n",
            "\n",
            "    Processing chunk 42/43: 0042.png\n",
            "      Using CONTEXT system prompt (Context length: ~918 chars).\n",
            "      Uploading: 0042.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~705 chars).\n",
            "\n",
            "    Processing chunk 43/43: 0043.png\n",
            "      Using CONTEXT system prompt (Context length: ~910 chars).\n",
            "      Uploading: 0043.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~618 chars).\n",
            "\n",
            "--- OCR Process Complete for Folder ---\n",
            "Results saved in: /content/Peut-on vivre en paix avec son inconscient  - bis_.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src = '/content/drive/MyDrive/folder_partitioned_merged/Le bonheur est-il affaire de hasard et de nécessité ? (AGREG interne -  note : 15,5)'\n",
        "dst = '/content/Le bonheur est-il affaire de hasard et de nécessité ? (AGREG interne -  note : 15,5)'\n",
        "\n",
        "if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "shutil.copytree(src, dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rnQ7Yv--8EoZ",
        "outputId": "962b8838-304b-450b-cbe1-2ce7e8fe516c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Le bonheur est-il affaire de hasard et de nécessité ? (AGREG interne -  note : 15,5)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OCR avec enrengistrement en CSV au même nom\n",
        "\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import csv\n",
        "import time\n",
        "import re # For natural sorting and filename sanitization\n",
        "from tqdm.notebook import tqdm # Progress bars\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# --- Configuration ---\n",
        "# *** SET THE TARGET FOLDER HERE ***\n",
        "TARGET_FOLDER_PATH = \"/content/Langage et réalité (AGREG ext. 2018 - note : 19)\" # Example: Set to the specific subfolder path you want to process\n",
        "\n",
        "# Output CSV file path will be generated dynamically later\n",
        "\n",
        "# Gemini API Key (ensure it's set as an environment variable)\n",
        "# ... (API Key configuration remains the same) ...\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "    print(\"Gemini API Key configured.\")\n",
        "except KeyError:\n",
        "    print(\"🛑 Error: GEMINI_API_KEY environment variable not set.\")\n",
        "    # sys.exit(1) # Optional: exit if running unattended\n",
        "except Exception as e:\n",
        "    print(f\"🛑 Error configuring Gemini API: {e}\")\n",
        "    # sys.exit(1) # Optional: exit if running unattended\n",
        "\n",
        "\n",
        "# Gemini Model Configuration)\n",
        "GENERATION_CONFIG = {\n",
        "  \"temperature\": 0.2, # Slightly lower temp might encourage more fidelity\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "MODEL_NAME = \"gemini-2.5-pro-preview-03-25\"\n",
        "\n",
        "\n",
        "# System Prompts (remain the same)\n",
        "SYSTEM_INSTRUCTION_WITH_CONTEXT = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\".\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\n",
        "Aide Contextuelle (Texte Précédent) : Pour t'aider à déchiffrer l'écriture, le texte qui précède *immédiatement* celui de l'image actuelle est fourni ci-dessous (séparé par '--- Texte Précédent ---'). Ce texte a été obtenu par OCR des sections précédentes de la dissertation. Le texte de l'image actuelle commence là où le texte précédent s'arrête (il peut y avoir un léger chevauchement de lignes dû au découpage des images).\n",
        "\n",
        "**Priorité Absolue :** Ta tâche principale reste de transcrire *exactement* ce que tu vois sur l'image actuelle. Utilise le texte précédent **uniquement** comme une aide *secondaire* pour résoudre des ambiguïtés ou déchiffrer des mots très difficiles. **Ne laisse PAS le texte précédent remplacer ou modifier ce qui est clairement visible sur l'image actuelle.** En cas de conflit entre le texte précédent et l'image, **la transcription fidèle de l'image PRÉVAUT.**\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_INSTRUCTION_FIRST_CHUNK = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\". Ceci est le tout début de la dissertation.\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\"\"\"\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def extract_folder_info(folder_path):\n",
        "    \"\"\"Extracts the subject and full name from a folder path name.\"\"\"\n",
        "    if not os.path.isdir(folder_path): return None, None\n",
        "    full_name = os.path.basename(folder_path)\n",
        "    subject = None\n",
        "    index_parenthese = full_name.find('(')\n",
        "    if index_parenthese != -1: subject = full_name[:index_parenthese].strip()\n",
        "    else: subject = full_name.strip() # Fallback\n",
        "    return subject, full_name\n",
        "\n",
        "# --- NEW: Function to sanitize filename ---\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Removes or replaces characters invalid for filenames.\"\"\"\n",
        "    # Remove leading/trailing whitespace\n",
        "    sanitized = filename.strip()\n",
        "    # Replace common invalid characters with underscore (adjust as needed)\n",
        "    sanitized = re.sub(r'[\\\\/*?:\"<>|]', '_', sanitized)\n",
        "    # Replace forward slash specifically (common in paths)\n",
        "    sanitized = sanitized.replace('/', '_')\n",
        "    # Optional: Limit length if necessary (e.g., 200 chars)\n",
        "    # max_len = 200\n",
        "    # if len(sanitized) > max_len:\n",
        "    #     name, ext = os.path.splitext(sanitized)\n",
        "    #     sanitized = name[:max_len - len(ext) - 1] + '~' + ext\n",
        "    return sanitized\n",
        "\n",
        "\n",
        "# ... (upload_to_gemini, delete_gemini_file, natural_sort_key remain the same) ...\n",
        "def upload_to_gemini(path, mime_type=\"image/png\"):\n",
        "    \"\"\"Uploads a file to Gemini and returns the File object.\"\"\"\n",
        "    try:\n",
        "        file = genai.upload_file(path, mime_type=mime_type)\n",
        "        return file\n",
        "    except Exception as e:\n",
        "        print(f\"  🛑 Error uploading file {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def delete_gemini_file(file_object):\n",
        "    \"\"\"Deletes a file from Gemini service.\"\"\"\n",
        "    if file_object:\n",
        "        try: genai.delete_file(file_object.name)\n",
        "        except Exception as e: print(f\"  ⚠️ Warning: Failed to delete file {file_object.name}: {e}\")\n",
        "\n",
        "def natural_sort_key(s):\n",
        "    \"\"\"Key for sorting strings containing numbers naturally.\"\"\"\n",
        "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
        "\n",
        "\n",
        "# --- Main Processing Logic for Single Folder ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.isdir(TARGET_FOLDER_PATH):\n",
        "        print(f\"🛑 Error: Target folder not found at '{TARGET_FOLDER_PATH}'\")\n",
        "    else:\n",
        "        print(f\"Starting OCR process for single folder: '{os.path.basename(TARGET_FOLDER_PATH)}'\")\n",
        "\n",
        "        # 1. Extract Subject and Full Name for the target folder\n",
        "        sujet_copie, nom_complet_sous_dossier = extract_folder_info(TARGET_FOLDER_PATH)\n",
        "\n",
        "        if sujet_copie is None:\n",
        "            print(f\"🛑 Error: Could not extract info from folder path '{TARGET_FOLDER_PATH}'. Cannot proceed.\")\n",
        "        else:\n",
        "            print(f\"  Subject detected: '{sujet_copie}'\")\n",
        "\n",
        "            # --- GENERATE DYNAMIC OUTPUT CSV FILENAME ---\n",
        "            parent_dir = os.path.dirname(TARGET_FOLDER_PATH) # Get directory containing the target folder\n",
        "            sanitized_subject = sanitize_filename(sujet_copie) # Sanitize the subject for use in filename\n",
        "            OUTPUT_CSV_FILE = os.path.join(parent_dir, f\"{sanitized_subject}.csv\")\n",
        "            print(f\"  Output CSV will be saved as: '{OUTPUT_CSV_FILE}'\")\n",
        "            # --- End Filename Generation ---\n",
        "\n",
        "            # --- Initialize CSV Writer ---\n",
        "            try:\n",
        "                with open(OUTPUT_CSV_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                    writer = csv.writer(csvfile)\n",
        "                    print(\"  CSV file opened successfully.\")\n",
        "\n",
        "                    # Write the folder name (title) to the CSV\n",
        "                    writer.writerow([nom_complet_sous_dossier])\n",
        "                    print(f\"  Wrote folder title to CSV: {nom_complet_sous_dossier}\")\n",
        "\n",
        "                    # 2. Find and Sort Image Chunks\n",
        "                    # ... (Chunk finding logic remains the same) ...\n",
        "                    try:\n",
        "                        all_files = [f for f in os.listdir(TARGET_FOLDER_PATH) if f.lower().endswith(\".png\")]\n",
        "                        sorted_chunks = sorted(all_files, key=natural_sort_key)\n",
        "                    except OSError as e:\n",
        "                        print(f\"  🛑 Error listing files in {TARGET_FOLDER_PATH}: {e}\")\n",
        "                        sorted_chunks = []\n",
        "\n",
        "\n",
        "                    if not sorted_chunks:\n",
        "                        print(\"  No PNG chunk files found in this folder.\")\n",
        "                    else:\n",
        "                        print(f\"  Found {len(sorted_chunks)} chunk(s) to process.\")\n",
        "\n",
        "                        # --- Variables to store previous OCR results ---\n",
        "                        ocr_result_n_minus_1 = \"\"\n",
        "                        ocr_result_n_minus_2 = \"\"\n",
        "\n",
        "                        # --- Loop through Image Chunks ---\n",
        "                        # ... (The rest of the loop logic remains exactly the same as the previous version) ...\n",
        "                        for i, chunk_filename in enumerate(tqdm(sorted_chunks, desc=\"Processing Chunks\")):\n",
        "                            chunk_path = os.path.join(TARGET_FOLDER_PATH, chunk_filename)\n",
        "                            gemini_file = None\n",
        "                            current_ocr_result = \"\" # Initialize for this chunk\n",
        "\n",
        "                            print(f\"\\n    Processing chunk {i+1}/{len(sorted_chunks)}: {chunk_filename}\")\n",
        "\n",
        "                            try:\n",
        "                                # 3. Select System Prompt and Prepare Context\n",
        "                                prompt_parts = []\n",
        "                                if i == 0:\n",
        "                                    # First chunk: No preceding context\n",
        "                                    current_system_instruction = SYSTEM_INSTRUCTION_FIRST_CHUNK.format(sujet_copie=sujet_copie)\n",
        "                                    print(\"      Using FIRST chunk system prompt.\")\n",
        "                                else:\n",
        "                                    # Subsequent chunks: Prepare context text\n",
        "                                    current_system_instruction = SYSTEM_INSTRUCTION_WITH_CONTEXT.format(sujet_copie=sujet_copie)\n",
        "                                    context_text = \"\\n--- Texte Précédent ---\\n\"\n",
        "                                    if ocr_result_n_minus_2: # Add N-2 if available\n",
        "                                        context_text += ocr_result_n_minus_2 + \"\\n\" # Add newline separator\n",
        "                                    if ocr_result_n_minus_1: # Add N-1 if available\n",
        "                                        context_text += ocr_result_n_minus_1\n",
        "                                    context_text += \"\\n--- Fin du Texte Précédent ---\\n\\n--- Image Actuelle à Transcrire ---\"\n",
        "                                    # Add context text as the first part of the prompt\n",
        "                                    prompt_parts.append(context_text)\n",
        "                                    print(f\"      Using CONTEXT system prompt (Context length: ~{len(context_text)} chars).\")\n",
        "\n",
        "\n",
        "                                # 4. Upload Image Chunk\n",
        "                                print(f\"      Uploading: {chunk_filename}...\")\n",
        "                                gemini_file = upload_to_gemini(chunk_path)\n",
        "                                if gemini_file is None:\n",
        "                                    current_ocr_result = f\"ERROR: Upload failed for {chunk_filename}\"\n",
        "                                    writer.writerow([current_ocr_result])\n",
        "                                    print(f\"      {current_ocr_result}\")\n",
        "                                    continue # Skip to next chunk\n",
        "\n",
        "                                # Add the image file as the next part of the prompt\n",
        "                                prompt_parts.append(gemini_file)\n",
        "\n",
        "                                # 5. Initialize Model (Can potentially be outside loop if system prompt doesn't change often, but safer inside for now)\n",
        "                                # Re-create model instance ensures the correct system prompt is used\n",
        "                                model = genai.GenerativeModel(\n",
        "                                    model_name=MODEL_NAME,\n",
        "                                    generation_config=GENERATION_CONFIG,\n",
        "                                    system_instruction=current_system_instruction,\n",
        "                                )\n",
        "\n",
        "                                # 6. Call Gemini API\n",
        "                                print(\"      Calling Gemini API...\")\n",
        "                                response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
        "\n",
        "                                # 7. Extract OCR Result\n",
        "                                if response and response.parts:\n",
        "                                     try:\n",
        "                                         current_ocr_result = response.text\n",
        "                                         # Optional: Remove potential leading/trailing whitespace\n",
        "                                         current_ocr_result = current_ocr_result.strip()\n",
        "                                         print(f\"      OCR result received (~{len(current_ocr_result)} chars).\")\n",
        "                                     except ValueError:\n",
        "                                         print(f\"      ⚠️ Warning: Could not extract text from response for {chunk_filename}. Response: {response.prompt_feedback}\")\n",
        "                                         current_ocr_result = f\"ERROR: No text found or content blocked ({response.prompt_feedback})\"\n",
        "                                     except Exception as e_resp:\n",
        "                                        print(f\"      🛑 Error extracting text from response: {e_resp}\")\n",
        "                                        current_ocr_result = f\"ERROR: Response parsing failed - {e_resp}\"\n",
        "                                else:\n",
        "                                    print(f\"      ⚠️ Warning: Empty or invalid response received for {chunk_filename}.\")\n",
        "                                    current_ocr_result = \"ERROR: Empty or invalid response from API\"\n",
        "\n",
        "                                # Write result to CSV\n",
        "                                writer.writerow([current_ocr_result])\n",
        "                                # print(f\"      Wrote OCR result to CSV.\")\n",
        "\n",
        "                                # 8. Update History for Next Iteration\n",
        "                                ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                                ocr_result_n_minus_1 = current_ocr_result # Store the raw result from this chunk\n",
        "\n",
        "                            except Exception as e_api:\n",
        "                                print(f\"    🛑 An error occurred during API call or processing for {chunk_filename}:\")\n",
        "                                traceback.print_exc() # Print full traceback for debugging\n",
        "                                error_message = f\"ERROR: API call failed for {chunk_filename} - {type(e_api).__name__}\"\n",
        "                                writer.writerow([error_message])\n",
        "                                # Update history with error marker to avoid propagating bad context\n",
        "                                ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                                ocr_result_n_minus_1 = error_message # Store error marker\n",
        "\n",
        "                            finally:\n",
        "                                # 9. Delete Uploaded File from Gemini\n",
        "                                if gemini_file:\n",
        "                                    delete_gemini_file(gemini_file)\n",
        "                                time.sleep(1.5) # Slightly increased delay\n",
        "\n",
        "\n",
        "            except IOError as e:\n",
        "                print(f\"🛑 Error opening or writing to CSV file {OUTPUT_CSV_FILE}: {e}\")\n",
        "            except Exception as e_main:\n",
        "                 print(f\"🛑 An unexpected error occurred during the main process:\")\n",
        "                 traceback.print_exc()\n",
        "\n",
        "            print(\"\\n--- OCR Process Complete for Folder ---\")\n",
        "            print(f\"Results saved in: {OUTPUT_CSV_FILE}\") # Display the final generated path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8707f2c37cb44e3d9cd1c19d2d826b57",
            "e0a100db45054d6a832e159cd2b1da79",
            "be9361b072d240b5bac76580857dba76",
            "4c5e5642f1ac4fb0ac5371a623ab2612",
            "886894204e2843c4b41350ec06bcc72b",
            "6f304d4444eb4f0c923e25a07873da68",
            "5014923c3eb94630939c1a9db0e76893",
            "8aa6bf4c563644c0bd1ab8ce82dbe3fa",
            "8143492a567a49169a989f9039319369",
            "aa22cc218a8445f7a23e280d8d971eea",
            "c0c12db4fa13407b93de9b41e6d64aeb"
          ]
        },
        "cellView": "form",
        "id": "pIpaNjQ687vO",
        "outputId": "e82c52dc-71b5-4b1c-add9-044a211e326d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API Key configured.\n",
            "Starting OCR process for single folder: 'Langage et réalité (AGREG ext. 2018 - note : 19)'\n",
            "  Subject detected: 'Langage et réalité'\n",
            "  Output CSV will be saved as: '/content/Langage et réalité.csv'\n",
            "  CSV file opened successfully.\n",
            "  Wrote folder title to CSV: Langage et réalité (AGREG ext. 2018 - note : 19)\n",
            "  Found 52 chunk(s) to process.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Chunks:   0%|          | 0/52 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8707f2c37cb44e3d9cd1c19d2d826b57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Processing chunk 1/52: 0001.png\n",
            "      Using FIRST chunk system prompt.\n",
            "      Uploading: 0001.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~945 chars).\n",
            "\n",
            "    Processing chunk 2/52: 0002.png\n",
            "      Using CONTEXT system prompt (Context length: ~1038 chars).\n",
            "      Uploading: 0002.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~861 chars).\n",
            "\n",
            "    Processing chunk 3/52: 0003.png\n",
            "      Using CONTEXT system prompt (Context length: ~1900 chars).\n",
            "      Uploading: 0003.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~438 chars).\n",
            "\n",
            "    Processing chunk 4/52: 0004.png\n",
            "      Using CONTEXT system prompt (Context length: ~1393 chars).\n",
            "      Uploading: 0004.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~873 chars).\n",
            "\n",
            "    Processing chunk 5/52: 0005.png\n",
            "      Using CONTEXT system prompt (Context length: ~1405 chars).\n",
            "      Uploading: 0005.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~835 chars).\n",
            "\n",
            "    Processing chunk 6/52: 0006.png\n",
            "      Using CONTEXT system prompt (Context length: ~1802 chars).\n",
            "      Uploading: 0006.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~696 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/5i4kxdigdyrt: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "    Processing chunk 7/52: 0007.png\n",
            "      Using CONTEXT system prompt (Context length: ~1625 chars).\n",
            "      Uploading: 0007.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~717 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/ci1lay62e3w: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "    Processing chunk 8/52: 0008.png\n",
            "      Using CONTEXT system prompt (Context length: ~1507 chars).\n",
            "      Uploading: 0008.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~643 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/lrb3o6y91ym: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 9/52: 0009.png\n",
            "      Using CONTEXT system prompt (Context length: ~1454 chars).\n",
            "      Uploading: 0009.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~796 chars).\n",
            "\n",
            "    Processing chunk 10/52: 0010.png\n",
            "      Using CONTEXT system prompt (Context length: ~1533 chars).\n",
            "      Uploading: 0010.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~339 chars).\n",
            "\n",
            "    Processing chunk 11/52: 0011.png\n",
            "      Using CONTEXT system prompt (Context length: ~1229 chars).\n",
            "      Uploading: 0011.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~798 chars).\n",
            "\n",
            "    Processing chunk 12/52: 0012.png\n",
            "      Using CONTEXT system prompt (Context length: ~1231 chars).\n",
            "      Uploading: 0012.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~873 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/l3usj4ll0o5r: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 13/52: 0013.png\n",
            "      Using CONTEXT system prompt (Context length: ~1765 chars).\n",
            "      Uploading: 0013.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~824 chars).\n",
            "\n",
            "    Processing chunk 14/52: 0014.png\n",
            "      Using CONTEXT system prompt (Context length: ~1791 chars).\n",
            "      Uploading: 0014.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~278 chars).\n",
            "\n",
            "    Processing chunk 15/52: 0015.png\n",
            "      Using CONTEXT system prompt (Context length: ~1196 chars).\n",
            "      Uploading: 0015.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~810 chars).\n",
            "\n",
            "    Processing chunk 16/52: 0016.png\n",
            "      Using CONTEXT system prompt (Context length: ~1182 chars).\n",
            "      Uploading: 0016.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~753 chars).\n",
            "\n",
            "    Processing chunk 17/52: 0017.png\n",
            "      Using CONTEXT system prompt (Context length: ~1657 chars).\n",
            "      Uploading: 0017.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~389 chars).\n",
            "\n",
            "    Processing chunk 18/52: 0018.png\n",
            "      Using CONTEXT system prompt (Context length: ~1236 chars).\n",
            "      Uploading: 0018.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~839 chars).\n",
            "\n",
            "    Processing chunk 19/52: 0019.png\n",
            "      Using CONTEXT system prompt (Context length: ~1322 chars).\n",
            "      Uploading: 0019.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~817 chars).\n",
            "\n",
            "    Processing chunk 20/52: 0020.png\n",
            "      Using CONTEXT system prompt (Context length: ~1750 chars).\n",
            "      Uploading: 0020.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~655 chars).\n",
            "\n",
            "    Processing chunk 21/52: 0021.png\n",
            "      Using CONTEXT system prompt (Context length: ~1566 chars).\n",
            "      Uploading: 0021.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~608 chars).\n",
            "\n",
            "    Processing chunk 22/52: 0022.png\n",
            "      Using CONTEXT system prompt (Context length: ~1357 chars).\n",
            "      Uploading: 0022.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~654 chars).\n",
            "\n",
            "    Processing chunk 23/52: 0023.png\n",
            "      Using CONTEXT system prompt (Context length: ~1356 chars).\n",
            "      Uploading: 0023.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~783 chars).\n",
            "\n",
            "    Processing chunk 24/52: 0024.png\n",
            "      Using CONTEXT system prompt (Context length: ~1531 chars).\n",
            "      Uploading: 0024.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~326 chars).\n",
            "\n",
            "    Processing chunk 25/52: 0025.png\n",
            "      Using CONTEXT system prompt (Context length: ~1203 chars).\n",
            "      Uploading: 0025.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~793 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/zr94j5w9qzlk: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 26/52: 0026.png\n",
            "      Using CONTEXT system prompt (Context length: ~1213 chars).\n",
            "      Uploading: 0026.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~843 chars).\n",
            "\n",
            "    Processing chunk 27/52: 0027.png\n",
            "      Using CONTEXT system prompt (Context length: ~1730 chars).\n",
            "      Uploading: 0027.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~781 chars).\n",
            "\n",
            "    Processing chunk 28/52: 0028.png\n",
            "      Using CONTEXT system prompt (Context length: ~1718 chars).\n",
            "      Uploading: 0028.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~317 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/xy37n0e3lohy: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "    Processing chunk 29/52: 0029.png\n",
            "      Using CONTEXT system prompt (Context length: ~1192 chars).\n",
            "      Uploading: 0029.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~772 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/m6d4fuw22pcw: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 30/52: 0030.png\n",
            "      Using CONTEXT system prompt (Context length: ~1183 chars).\n",
            "      Uploading: 0030.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~765 chars).\n",
            "\n",
            "    Processing chunk 31/52: 0031.png\n",
            "      Using CONTEXT system prompt (Context length: ~1631 chars).\n",
            "      Uploading: 0031.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~386 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/pz45o55elsc6: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 32/52: 0032.png\n",
            "      Using CONTEXT system prompt (Context length: ~1245 chars).\n",
            "      Uploading: 0032.png...\n",
            "      Calling Gemini API...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 583.36ms\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-51-f026a913f01f>\", line 218, in <cell line: 0>\n",
            "    response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota. Go to https://aistudio.google.com/apikey to upgrade your quota tier, or submit a quota increase request in https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    🛑 An error occurred during API call or processing for 0032.png:\n",
            "\n",
            "    Processing chunk 33/52: 0033.png\n",
            "      Using CONTEXT system prompt (Context length: ~533 chars).\n",
            "      Uploading: 0033.png...\n",
            "      Calling Gemini API...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 561.17ms\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-51-f026a913f01f>\", line 218, in <cell line: 0>\n",
            "    response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota. Go to https://aistudio.google.com/apikey to upgrade your quota tier, or submit a quota increase request in https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    🛑 An error occurred during API call or processing for 0033.png:\n",
            "\n",
            "    Processing chunk 34/52: 0034.png\n",
            "      Using CONTEXT system prompt (Context length: ~200 chars).\n",
            "      Uploading: 0034.png...\n",
            "      Calling Gemini API...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 583.89ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    🛑 An error occurred during API call or processing for 0034.png:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-51-f026a913f01f>\", line 218, in <cell line: 0>\n",
            "    response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota. Go to https://aistudio.google.com/apikey to upgrade your quota tier, or submit a quota increase request in https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Processing chunk 35/52: 0035.png\n",
            "      Using CONTEXT system prompt (Context length: ~200 chars).\n",
            "      Uploading: 0035.png...\n",
            "      Calling Gemini API...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 713.13ms\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-51-f026a913f01f>\", line 218, in <cell line: 0>\n",
            "    response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota. Go to https://aistudio.google.com/apikey to upgrade your quota tier, or submit a quota increase request in https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    🛑 An error occurred during API call or processing for 0035.png:\n",
            "\n",
            "    Processing chunk 36/52: 0036.png\n",
            "      Using CONTEXT system prompt (Context length: ~200 chars).\n",
            "      Uploading: 0036.png...\n",
            "      Calling Gemini API...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 988.05ms\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-51-f026a913f01f>\", line 218, in <cell line: 0>\n",
            "    response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota. Go to https://aistudio.google.com/apikey to upgrade your quota tier, or submit a quota increase request in https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    🛑 An error occurred during API call or processing for 0036.png:\n",
            "\n",
            "    Processing chunk 37/52: 0037.png\n",
            "      Using CONTEXT system prompt (Context length: ~200 chars).\n",
            "      Uploading: 0037.png...\n",
            "      Calling Gemini API...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 712.42ms\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-51-f026a913f01f>\", line 218, in <cell line: 0>\n",
            "    response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota. Go to https://aistudio.google.com/apikey to upgrade your quota tier, or submit a quota increase request in https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    🛑 An error occurred during API call or processing for 0037.png:\n",
            "\n",
            "    Processing chunk 38/52: 0038.png\n",
            "      Using CONTEXT system prompt (Context length: ~200 chars).\n",
            "      Uploading: 0038.png...\n",
            "      Calling Gemini API...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 687.21ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    🛑 An error occurred during API call or processing for 0038.png:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-51-f026a913f01f>\", line 218, in <cell line: 0>\n",
            "    response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota. Go to https://aistudio.google.com/apikey to upgrade your quota tier, or submit a quota increase request in https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Processing chunk 39/52: 0039.png\n",
            "      Using CONTEXT system prompt (Context length: ~200 chars).\n",
            "      Uploading: 0039.png...\n",
            "      Calling Gemini API...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-51-f026a913f01f>\", line 218, in <cell line: 0>\n",
            "    response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota. Go to https://aistudio.google.com/apikey to upgrade your quota tier, or submit a quota increase request in https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 855.31ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    🛑 An error occurred during API call or processing for 0039.png:\n",
            "\n",
            "    Processing chunk 40/52: 0040.png\n",
            "      Using CONTEXT system prompt (Context length: ~200 chars).\n",
            "      Uploading: 0040.png...\n",
            "      Calling Gemini API...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 584.28ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    🛑 An error occurred during API call or processing for 0040.png:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-51-f026a913f01f>\", line 218, in <cell line: 0>\n",
            "    response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota. Go to https://aistudio.google.com/apikey to upgrade your quota tier, or submit a quota increase request in https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Processing chunk 41/52: 0041.png\n",
            "      Using CONTEXT system prompt (Context length: ~200 chars).\n",
            "      Uploading: 0041.png...\n",
            "      Calling Gemini API...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 688.85ms\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-51-f026a913f01f>\", line 218, in <cell line: 0>\n",
            "    response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota. Go to https://aistudio.google.com/apikey to upgrade your quota tier, or submit a quota increase request in https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    🛑 An error occurred during API call or processing for 0041.png:\n",
            "\n",
            "    Processing chunk 42/52: 0042.png\n",
            "      Using CONTEXT system prompt (Context length: ~200 chars).\n",
            "      Uploading: 0042.png...\n",
            "      Calling Gemini API...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-51-f026a913f01f>\", line 218, in <cell line: 0>\n",
            "    response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.TooManyRequests: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota. Go to https://aistudio.google.com/apikey to upgrade your quota tier, or submit a quota increase request in https://ai.google.dev/gemini-api/docs/rate-limits#request-rate-limit-increase\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 664.33ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    🛑 An error occurred during API call or processing for 0042.png:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-f026a913f01f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mgemini_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                                     \u001b[0mdelete_gemini_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgemini_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Slightly increased delay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "src = '/content/drive/MyDrive/folder_partitioned_merged/Langage et réalité (AGREG ext. 2018 - note : 19)'\n",
        "dst = '/content/Langage et réalité (AGREG ext. 2018 - note : 19)'\n",
        "\n",
        "if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "\n",
        "shutil.copytree(src, dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZpnLlpX4CP-8",
        "outputId": "df84bb68-967f-4221-ea50-11bde7977e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/Langage et réalité (AGREG ext. 2018 - note : 19)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title OCR avec enrengistrement en CSV au même nom\n",
        "\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import csv\n",
        "import time\n",
        "import re # For natural sorting and filename sanitization\n",
        "from tqdm.notebook import tqdm # Progress bars\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# --- Configuration ---\n",
        "# *** SET THE TARGET FOLDER HERE ***\n",
        "TARGET_FOLDER_PATH = \"/content/Langage et réalité (AGREG ext. 2018 - note : 19)\" # Example: Set to the specific subfolder path you want to process\n",
        "\n",
        "# Output CSV file path will be generated dynamically later\n",
        "\n",
        "# Gemini API Key (ensure it's set as an environment variable)\n",
        "# ... (API Key configuration remains the same) ...\n",
        "try:\n",
        "    genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "    print(\"Gemini API Key configured.\")\n",
        "except KeyError:\n",
        "    print(\"🛑 Error: GEMINI_API_KEY environment variable not set.\")\n",
        "    # sys.exit(1) # Optional: exit if running unattended\n",
        "except Exception as e:\n",
        "    print(f\"🛑 Error configuring Gemini API: {e}\")\n",
        "    # sys.exit(1) # Optional: exit if running unattended\n",
        "\n",
        "\n",
        "# Gemini Model Configuration)\n",
        "GENERATION_CONFIG = {\n",
        "  \"temperature\": 0.2, # Slightly lower temp might encourage more fidelity\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "MODEL_NAME = \"gemini-2.5-pro-preview-03-25\"\n",
        "\n",
        "\n",
        "# System Prompts (remain the same)\n",
        "SYSTEM_INSTRUCTION_WITH_CONTEXT = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\".\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\n",
        "Aide Contextuelle (Texte Précédent) : Pour t'aider à déchiffrer l'écriture, le texte qui précède *immédiatement* celui de l'image actuelle est fourni ci-dessous (séparé par '--- Texte Précédent ---'). Ce texte a été obtenu par OCR des sections précédentes de la dissertation. Le texte de l'image actuelle commence là où le texte précédent s'arrête (il peut y avoir un léger chevauchement de lignes dû au découpage des images).\n",
        "\n",
        "**Priorité Absolue :** Ta tâche principale reste de transcrire *exactement* ce que tu vois sur l'image actuelle.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_INSTRUCTION_FIRST_CHUNK = \"\"\"Tâche Principale : Recopie aussi fidèlement que possible le texte manuscrit de l'image fournie. Répond **exclusivement** avec le texte transcrit, sans aucune introduction, commentaire, ou note.\n",
        "\n",
        "Contexte de la Dissertation : Ce texte est un extrait d'une dissertation de philosophie portant sur le sujet \"{sujet_copie}\". Ceci est le tout début de la dissertation.\n",
        "\n",
        "Mise en forme :\n",
        "- Si des mots ou groupes de mots sont soulignés dans l'image, mets-les en italique en markdown (*texte souligné*).\n",
        "- Si un mot est barré, encadre-le de tildes en markdown (~mot barré~).\n",
        "\"\"\"\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def extract_folder_info(folder_path):\n",
        "    \"\"\"Extracts the subject and full name from a folder path name.\"\"\"\n",
        "    if not os.path.isdir(folder_path): return None, None\n",
        "    full_name = os.path.basename(folder_path)\n",
        "    subject = None\n",
        "    index_parenthese = full_name.find('(')\n",
        "    if index_parenthese != -1: subject = full_name[:index_parenthese].strip()\n",
        "    else: subject = full_name.strip() # Fallback\n",
        "    return subject, full_name\n",
        "\n",
        "# --- NEW: Function to sanitize filename ---\n",
        "def sanitize_filename(filename):\n",
        "    \"\"\"Removes or replaces characters invalid for filenames.\"\"\"\n",
        "    # Remove leading/trailing whitespace\n",
        "    sanitized = filename.strip()\n",
        "    # Replace common invalid characters with underscore (adjust as needed)\n",
        "    sanitized = re.sub(r'[\\\\/*?:\"<>|]', '_', sanitized)\n",
        "    # Replace forward slash specifically (common in paths)\n",
        "    sanitized = sanitized.replace('/', '_')\n",
        "    # Optional: Limit length if necessary (e.g., 200 chars)\n",
        "    # max_len = 200\n",
        "    # if len(sanitized) > max_len:\n",
        "    #     name, ext = os.path.splitext(sanitized)\n",
        "    #     sanitized = name[:max_len - len(ext) - 1] + '~' + ext\n",
        "    return sanitized\n",
        "\n",
        "\n",
        "# ... (upload_to_gemini, delete_gemini_file, natural_sort_key remain the same) ...\n",
        "def upload_to_gemini(path, mime_type=\"image/png\"):\n",
        "    \"\"\"Uploads a file to Gemini and returns the File object.\"\"\"\n",
        "    try:\n",
        "        file = genai.upload_file(path, mime_type=mime_type)\n",
        "        return file\n",
        "    except Exception as e:\n",
        "        print(f\"  🛑 Error uploading file {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def delete_gemini_file(file_object):\n",
        "    \"\"\"Deletes a file from Gemini service.\"\"\"\n",
        "    if file_object:\n",
        "        try: genai.delete_file(file_object.name)\n",
        "        except Exception as e: print(f\"  ⚠️ Warning: Failed to delete file {file_object.name}: {e}\")\n",
        "\n",
        "def natural_sort_key(s):\n",
        "    \"\"\"Key for sorting strings containing numbers naturally.\"\"\"\n",
        "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
        "\n",
        "\n",
        "# --- Main Processing Logic for Single Folder ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.isdir(TARGET_FOLDER_PATH):\n",
        "        print(f\"🛑 Error: Target folder not found at '{TARGET_FOLDER_PATH}'\")\n",
        "    else:\n",
        "        print(f\"Starting OCR process for single folder: '{os.path.basename(TARGET_FOLDER_PATH)}'\")\n",
        "\n",
        "        # 1. Extract Subject and Full Name for the target folder\n",
        "        sujet_copie, nom_complet_sous_dossier = extract_folder_info(TARGET_FOLDER_PATH)\n",
        "\n",
        "        if sujet_copie is None:\n",
        "            print(f\"🛑 Error: Could not extract info from folder path '{TARGET_FOLDER_PATH}'. Cannot proceed.\")\n",
        "        else:\n",
        "            print(f\"  Subject detected: '{sujet_copie}'\")\n",
        "\n",
        "            # --- GENERATE DYNAMIC OUTPUT CSV FILENAME ---\n",
        "            parent_dir = os.path.dirname(TARGET_FOLDER_PATH) # Get directory containing the target folder\n",
        "            sanitized_subject = sanitize_filename(sujet_copie) # Sanitize the subject for use in filename\n",
        "            OUTPUT_CSV_FILE = os.path.join(parent_dir, f\"{sanitized_subject}.csv\")\n",
        "            print(f\"  Output CSV will be saved as: '{OUTPUT_CSV_FILE}'\")\n",
        "            # --- End Filename Generation ---\n",
        "\n",
        "            # --- Initialize CSV Writer ---\n",
        "            try:\n",
        "                with open(OUTPUT_CSV_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                    writer = csv.writer(csvfile)\n",
        "                    print(\"  CSV file opened successfully.\")\n",
        "\n",
        "                    # Write the folder name (title) to the CSV\n",
        "                    writer.writerow([nom_complet_sous_dossier])\n",
        "                    print(f\"  Wrote folder title to CSV: {nom_complet_sous_dossier}\")\n",
        "\n",
        "                    # 2. Find and Sort Image Chunks\n",
        "                    # ... (Chunk finding logic remains the same) ...\n",
        "                    try:\n",
        "                        all_files = [f for f in os.listdir(TARGET_FOLDER_PATH) if f.lower().endswith(\".png\")]\n",
        "                        sorted_chunks = sorted(all_files, key=natural_sort_key)\n",
        "                    except OSError as e:\n",
        "                        print(f\"  🛑 Error listing files in {TARGET_FOLDER_PATH}: {e}\")\n",
        "                        sorted_chunks = []\n",
        "\n",
        "\n",
        "                    if not sorted_chunks:\n",
        "                        print(\"  No PNG chunk files found in this folder.\")\n",
        "                    else:\n",
        "                        print(f\"  Found {len(sorted_chunks)} chunk(s) to process.\")\n",
        "\n",
        "                        # --- Variables to store previous OCR results ---\n",
        "                        ocr_result_n_minus_1 = \"\"\n",
        "                        ocr_result_n_minus_2 = \"\"\n",
        "\n",
        "                        # --- Loop through Image Chunks ---\n",
        "                        # ... (The rest of the loop logic remains exactly the same as the previous version) ...\n",
        "                        for i, chunk_filename in enumerate(tqdm(sorted_chunks, desc=\"Processing Chunks\")):\n",
        "                            chunk_path = os.path.join(TARGET_FOLDER_PATH, chunk_filename)\n",
        "                            gemini_file = None\n",
        "                            current_ocr_result = \"\" # Initialize for this chunk\n",
        "\n",
        "                            print(f\"\\n    Processing chunk {i+1}/{len(sorted_chunks)}: {chunk_filename}\")\n",
        "\n",
        "                            try:\n",
        "                                # 3. Select System Prompt and Prepare Context\n",
        "                                prompt_parts = []\n",
        "                                if i == 0:\n",
        "                                    # First chunk: No preceding context\n",
        "                                    current_system_instruction = SYSTEM_INSTRUCTION_FIRST_CHUNK.format(sujet_copie=sujet_copie)\n",
        "                                    print(\"      Using FIRST chunk system prompt.\")\n",
        "                                else:\n",
        "                                    # Subsequent chunks: Prepare context text\n",
        "                                    current_system_instruction = SYSTEM_INSTRUCTION_WITH_CONTEXT.format(sujet_copie=sujet_copie)\n",
        "                                    context_text = \"\\n--- Texte Précédent ---\\n\"\n",
        "                                    if ocr_result_n_minus_2: # Add N-2 if available\n",
        "                                        context_text += ocr_result_n_minus_2 + \"\\n\" # Add newline separator\n",
        "                                    if ocr_result_n_minus_1: # Add N-1 if available\n",
        "                                        context_text += ocr_result_n_minus_1\n",
        "                                    context_text += \"\\n--- Fin du Texte Précédent ---\\n\\n--- Image Actuelle à Transcrire ---\"\n",
        "                                    # Add context text as the first part of the prompt\n",
        "                                    prompt_parts.append(context_text)\n",
        "                                    print(f\"      Using CONTEXT system prompt (Context length: ~{len(context_text)} chars).\")\n",
        "\n",
        "\n",
        "                                # 4. Upload Image Chunk\n",
        "                                print(f\"      Uploading: {chunk_filename}...\")\n",
        "                                gemini_file = upload_to_gemini(chunk_path)\n",
        "                                if gemini_file is None:\n",
        "                                    current_ocr_result = f\"ERROR: Upload failed for {chunk_filename}\"\n",
        "                                    writer.writerow([current_ocr_result])\n",
        "                                    print(f\"      {current_ocr_result}\")\n",
        "                                    continue # Skip to next chunk\n",
        "\n",
        "                                # Add the image file as the next part of the prompt\n",
        "                                prompt_parts.append(gemini_file)\n",
        "\n",
        "                                # 5. Initialize Model (Can potentially be outside loop if system prompt doesn't change often, but safer inside for now)\n",
        "                                # Re-create model instance ensures the correct system prompt is used\n",
        "                                model = genai.GenerativeModel(\n",
        "                                    model_name=MODEL_NAME,\n",
        "                                    generation_config=GENERATION_CONFIG,\n",
        "                                    system_instruction=current_system_instruction,\n",
        "                                )\n",
        "\n",
        "                                # 6. Call Gemini API\n",
        "                                print(\"      Calling Gemini API...\")\n",
        "                                response = model.generate_content(prompt_parts, request_options={'timeout': 600}) # Increased timeout\n",
        "\n",
        "                                # 7. Extract OCR Result\n",
        "                                if response and response.parts:\n",
        "                                     try:\n",
        "                                         current_ocr_result = response.text\n",
        "                                         # Optional: Remove potential leading/trailing whitespace\n",
        "                                         current_ocr_result = current_ocr_result.strip()\n",
        "                                         print(f\"      OCR result received (~{len(current_ocr_result)} chars).\")\n",
        "                                     except ValueError:\n",
        "                                         print(f\"      ⚠️ Warning: Could not extract text from response for {chunk_filename}. Response: {response.prompt_feedback}\")\n",
        "                                         current_ocr_result = f\"ERROR: No text found or content blocked ({response.prompt_feedback})\"\n",
        "                                     except Exception as e_resp:\n",
        "                                        print(f\"      🛑 Error extracting text from response: {e_resp}\")\n",
        "                                        current_ocr_result = f\"ERROR: Response parsing failed - {e_resp}\"\n",
        "                                else:\n",
        "                                    print(f\"      ⚠️ Warning: Empty or invalid response received for {chunk_filename}.\")\n",
        "                                    current_ocr_result = \"ERROR: Empty or invalid response from API\"\n",
        "\n",
        "                                # Write result to CSV\n",
        "                                writer.writerow([current_ocr_result])\n",
        "                                # print(f\"      Wrote OCR result to CSV.\")\n",
        "\n",
        "                                # 8. Update History for Next Iteration\n",
        "                                ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                                ocr_result_n_minus_1 = current_ocr_result # Store the raw result from this chunk\n",
        "\n",
        "                            except Exception as e_api:\n",
        "                                print(f\"    🛑 An error occurred during API call or processing for {chunk_filename}:\")\n",
        "                                traceback.print_exc() # Print full traceback for debugging\n",
        "                                error_message = f\"ERROR: API call failed for {chunk_filename} - {type(e_api).__name__}\"\n",
        "                                writer.writerow([error_message])\n",
        "                                # Update history with error marker to avoid propagating bad context\n",
        "                                ocr_result_n_minus_2 = ocr_result_n_minus_1\n",
        "                                ocr_result_n_minus_1 = error_message # Store error marker\n",
        "\n",
        "                            finally:\n",
        "                                # 9. Delete Uploaded File from Gemini\n",
        "                                if gemini_file:\n",
        "                                    delete_gemini_file(gemini_file)\n",
        "                                time.sleep(1.5) # Slightly increased delay\n",
        "\n",
        "\n",
        "            except IOError as e:\n",
        "                print(f\"🛑 Error opening or writing to CSV file {OUTPUT_CSV_FILE}: {e}\")\n",
        "            except Exception as e_main:\n",
        "                 print(f\"🛑 An unexpected error occurred during the main process:\")\n",
        "                 traceback.print_exc()\n",
        "\n",
        "            print(\"\\n--- OCR Process Complete for Folder ---\")\n",
        "            print(f\"Results saved in: {OUTPUT_CSV_FILE}\") # Display the final generated path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "73c4b6cfcda2494eb21ce068305b1280",
            "6abb5c3be45c4d6c949c84a71c5abdc1",
            "78db113cb3a44bf4a698ff8b10e599f6",
            "07754be44120482fb339b951249e6aa9",
            "d86c690556704626ad52ad52b883e843",
            "e8ee4fcfb46544eca39974b49e1d05ad",
            "70d5da0408e84e5e8f2a128d29576ea5",
            "b59fc2c3a4ce4f9cb7bea65354c010aa",
            "ba66cbc728c94a22b5dd36b29a442dbf",
            "71a432ccc1e14a0691446c652e657fed",
            "558ad718b9894647b578fa3951f8395f"
          ]
        },
        "id": "s4XwqFg-COkL",
        "outputId": "b0799435-01a1-42be-c7f3-173d2d126de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini API Key configured.\n",
            "Starting OCR process for single folder: 'Langage et réalité (AGREG ext. 2018 - note : 19)'\n",
            "  Subject detected: 'Langage et réalité'\n",
            "  Output CSV will be saved as: '/content/Langage et réalité.csv'\n",
            "  CSV file opened successfully.\n",
            "  Wrote folder title to CSV: Langage et réalité (AGREG ext. 2018 - note : 19)\n",
            "  Found 52 chunk(s) to process.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Chunks:   0%|          | 0/52 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73c4b6cfcda2494eb21ce068305b1280"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Processing chunk 1/52: 0001.png\n",
            "      Using FIRST chunk system prompt.\n",
            "      Uploading: 0001.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~945 chars).\n",
            "\n",
            "    Processing chunk 2/52: 0002.png\n",
            "      Using CONTEXT system prompt (Context length: ~1038 chars).\n",
            "      Uploading: 0002.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~863 chars).\n",
            "\n",
            "    Processing chunk 3/52: 0003.png\n",
            "      Using CONTEXT system prompt (Context length: ~1902 chars).\n",
            "      Uploading: 0003.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~440 chars).\n",
            "\n",
            "    Processing chunk 4/52: 0004.png\n",
            "      Using CONTEXT system prompt (Context length: ~1397 chars).\n",
            "      Uploading: 0004.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~877 chars).\n",
            "\n",
            "    Processing chunk 5/52: 0005.png\n",
            "      Using CONTEXT system prompt (Context length: ~1411 chars).\n",
            "      Uploading: 0005.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~837 chars).\n",
            "\n",
            "    Processing chunk 6/52: 0006.png\n",
            "      Using CONTEXT system prompt (Context length: ~1808 chars).\n",
            "      Uploading: 0006.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~696 chars).\n",
            "\n",
            "    Processing chunk 7/52: 0007.png\n",
            "      Using CONTEXT system prompt (Context length: ~1627 chars).\n",
            "      Uploading: 0007.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~717 chars).\n",
            "\n",
            "    Processing chunk 8/52: 0008.png\n",
            "      Using CONTEXT system prompt (Context length: ~1507 chars).\n",
            "      Uploading: 0008.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~571 chars).\n",
            "\n",
            "    Processing chunk 9/52: 0009.png\n",
            "      Using CONTEXT system prompt (Context length: ~1382 chars).\n",
            "      Uploading: 0009.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~796 chars).\n",
            "\n",
            "    Processing chunk 10/52: 0010.png\n",
            "      Using CONTEXT system prompt (Context length: ~1461 chars).\n",
            "      Uploading: 0010.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~339 chars).\n",
            "\n",
            "    Processing chunk 11/52: 0011.png\n",
            "      Using CONTEXT system prompt (Context length: ~1229 chars).\n",
            "      Uploading: 0011.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~798 chars).\n",
            "\n",
            "    Processing chunk 12/52: 0012.png\n",
            "      Using CONTEXT system prompt (Context length: ~1231 chars).\n",
            "      Uploading: 0012.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~873 chars).\n",
            "\n",
            "    Processing chunk 13/52: 0013.png\n",
            "      Using CONTEXT system prompt (Context length: ~1765 chars).\n",
            "      Uploading: 0013.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~825 chars).\n",
            "\n",
            "    Processing chunk 14/52: 0014.png\n",
            "      Using CONTEXT system prompt (Context length: ~1792 chars).\n",
            "      Uploading: 0014.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~279 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/d0yph20uril6: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 15/52: 0015.png\n",
            "      Using CONTEXT system prompt (Context length: ~1198 chars).\n",
            "      Uploading: 0015.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~810 chars).\n",
            "\n",
            "    Processing chunk 16/52: 0016.png\n",
            "      Using CONTEXT system prompt (Context length: ~1183 chars).\n",
            "      Uploading: 0016.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~817 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/by7ym52x0601: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 17/52: 0017.png\n",
            "      Using CONTEXT system prompt (Context length: ~1721 chars).\n",
            "      Uploading: 0017.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~390 chars).\n",
            "\n",
            "    Processing chunk 18/52: 0018.png\n",
            "      Using CONTEXT system prompt (Context length: ~1301 chars).\n",
            "      Uploading: 0018.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~838 chars).\n",
            "\n",
            "    Processing chunk 19/52: 0019.png\n",
            "      Using CONTEXT system prompt (Context length: ~1322 chars).\n",
            "      Uploading: 0019.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~817 chars).\n",
            "\n",
            "    Processing chunk 20/52: 0020.png\n",
            "      Using CONTEXT system prompt (Context length: ~1749 chars).\n",
            "      Uploading: 0020.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~655 chars).\n",
            "\n",
            "    Processing chunk 21/52: 0021.png\n",
            "      Using CONTEXT system prompt (Context length: ~1566 chars).\n",
            "      Uploading: 0021.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~608 chars).\n",
            "\n",
            "    Processing chunk 22/52: 0022.png\n",
            "      Using CONTEXT system prompt (Context length: ~1357 chars).\n",
            "      Uploading: 0022.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~653 chars).\n",
            "\n",
            "    Processing chunk 23/52: 0023.png\n",
            "      Using CONTEXT system prompt (Context length: ~1355 chars).\n",
            "      Uploading: 0023.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~783 chars).\n",
            "\n",
            "    Processing chunk 24/52: 0024.png\n",
            "      Using CONTEXT system prompt (Context length: ~1530 chars).\n",
            "      Uploading: 0024.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~326 chars).\n",
            "\n",
            "    Processing chunk 25/52: 0025.png\n",
            "      Using CONTEXT system prompt (Context length: ~1203 chars).\n",
            "      Uploading: 0025.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~793 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/kmvpzv94a5iz: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 26/52: 0026.png\n",
            "      Using CONTEXT system prompt (Context length: ~1213 chars).\n",
            "      Uploading: 0026.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~847 chars).\n",
            "\n",
            "    Processing chunk 27/52: 0027.png\n",
            "      Using CONTEXT system prompt (Context length: ~1734 chars).\n",
            "      Uploading: 0027.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~778 chars).\n",
            "\n",
            "    Processing chunk 28/52: 0028.png\n",
            "      Using CONTEXT system prompt (Context length: ~1719 chars).\n",
            "      Uploading: 0028.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~256 chars).\n",
            "\n",
            "    Processing chunk 29/52: 0029.png\n",
            "      Using CONTEXT system prompt (Context length: ~1128 chars).\n",
            "      Uploading: 0029.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~776 chars).\n",
            "\n",
            "    Processing chunk 30/52: 0030.png\n",
            "      Using CONTEXT system prompt (Context length: ~1126 chars).\n",
            "      Uploading: 0030.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~774 chars).\n",
            "\n",
            "    Processing chunk 31/52: 0031.png\n",
            "      Using CONTEXT system prompt (Context length: ~1644 chars).\n",
            "      Uploading: 0031.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~386 chars).\n",
            "\n",
            "    Processing chunk 32/52: 0032.png\n",
            "      Using CONTEXT system prompt (Context length: ~1254 chars).\n",
            "      Uploading: 0032.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~804 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/ivtb776fmt64: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "    Processing chunk 33/52: 0033.png\n",
            "      Using CONTEXT system prompt (Context length: ~1284 chars).\n",
            "      Uploading: 0033.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~821 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/5480y8va054l: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "\n",
            "    Processing chunk 34/52: 0034.png\n",
            "      Using CONTEXT system prompt (Context length: ~1719 chars).\n",
            "      Uploading: 0034.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~657 chars).\n",
            "\n",
            "    Processing chunk 35/52: 0035.png\n",
            "      Using CONTEXT system prompt (Context length: ~1572 chars).\n",
            "      Uploading: 0035.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~739 chars).\n",
            "\n",
            "    Processing chunk 36/52: 0036.png\n",
            "      Using CONTEXT system prompt (Context length: ~1490 chars).\n",
            "      Uploading: 0036.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~773 chars).\n",
            "\n",
            "    Processing chunk 37/52: 0037.png\n",
            "      Using CONTEXT system prompt (Context length: ~1606 chars).\n",
            "      Uploading: 0037.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~618 chars).\n",
            "\n",
            "    Processing chunk 38/52: 0038.png\n",
            "      Using CONTEXT system prompt (Context length: ~1485 chars).\n",
            "      Uploading: 0038.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~296 chars).\n",
            "\n",
            "    Processing chunk 39/52: 0039.png\n",
            "      Using CONTEXT system prompt (Context length: ~1008 chars).\n",
            "      Uploading: 0039.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~741 chars).\n",
            "\n",
            "    Processing chunk 40/52: 0040.png\n",
            "      Using CONTEXT system prompt (Context length: ~1131 chars).\n",
            "      Uploading: 0040.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~736 chars).\n",
            "\n",
            "    Processing chunk 41/52: 0041.png\n",
            "      Using CONTEXT system prompt (Context length: ~1571 chars).\n",
            "      Uploading: 0041.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~824 chars).\n",
            "\n",
            "    Processing chunk 42/52: 0042.png\n",
            "      Using CONTEXT system prompt (Context length: ~1654 chars).\n",
            "      Uploading: 0042.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~248 chars).\n",
            "  ⚠️ Warning: Failed to delete file files/aov5wzei1m3q: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "    Processing chunk 43/52: 0043.png\n",
            "      Using CONTEXT system prompt (Context length: ~1166 chars).\n",
            "      Uploading: 0043.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~829 chars).\n",
            "\n",
            "    Processing chunk 44/52: 0044.png\n",
            "      Using CONTEXT system prompt (Context length: ~1171 chars).\n",
            "      Uploading: 0044.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~722 chars).\n",
            "\n",
            "    Processing chunk 45/52: 0045.png\n",
            "      Using CONTEXT system prompt (Context length: ~1645 chars).\n",
            "      Uploading: 0045.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~410 chars).\n",
            "\n",
            "    Processing chunk 46/52: 0046.png\n",
            "      Using CONTEXT system prompt (Context length: ~1226 chars).\n",
            "      Uploading: 0046.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~813 chars).\n",
            "\n",
            "    Processing chunk 47/52: 0047.png\n",
            "      Using CONTEXT system prompt (Context length: ~1317 chars).\n",
            "      Uploading: 0047.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~787 chars).\n",
            "\n",
            "    Processing chunk 48/52: 0048.png\n",
            "      Using CONTEXT system prompt (Context length: ~1694 chars).\n",
            "      Uploading: 0048.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~279 chars).\n",
            "\n",
            "    Processing chunk 49/52: 0049.png\n",
            "      Using CONTEXT system prompt (Context length: ~1160 chars).\n",
            "      Uploading: 0049.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~701 chars).\n",
            "\n",
            "    Processing chunk 50/52: 0050.png\n",
            "      Using CONTEXT system prompt (Context length: ~1074 chars).\n",
            "      Uploading: 0050.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~697 chars).\n",
            "\n",
            "    Processing chunk 51/52: 0051.png\n",
            "      Using CONTEXT system prompt (Context length: ~1492 chars).\n",
            "      Uploading: 0051.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~784 chars).\n",
            "\n",
            "    Processing chunk 52/52: 0052.png\n",
            "      Using CONTEXT system prompt (Context length: ~1575 chars).\n",
            "      Uploading: 0052.png...\n",
            "      Calling Gemini API...\n",
            "      OCR result received (~122 chars).\n",
            "\n",
            "--- OCR Process Complete for Folder ---\n",
            "Results saved in: /content/Langage et réalité.csv\n"
          ]
        }
      ]
    }
  ]
}